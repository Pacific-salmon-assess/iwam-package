---
title: "Model Comparisons"
author: "Tor Kitching"
date: "2024-04-09"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
---

<!-- Note/comment syntax for html -->

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(nimble)
library(coda)
library(dplyr)
library(MCMCvis)
library(DHARMa)
library(ggplot2)
library(TMB)

library(rsample)
library(cowplot)
library(gridExtra)
library(reshape2)
library(zoo)
library(viridis)
library(hrbrthemes)
library(gsl)

library(knitr) # Tables
library(latex2exp) # For adding latex into plot labels

# library(geomtextpath)
```

```{r custom library, echo=FALSE, warning=FALSE, message=FALSE}
source(here::here("R/helperFunctions.R"))
source(here::here("R/PlotFunctions.R"))
source(here::here("R/IWAM_model.R"))
```

```{r custom Lambertw, echo=FALSE, warning=FALSE, message=FALSE}
## This seems like the best LambertsW function so far:
## https://github.com/cran/lamW/blob/master/R/lamW.R
nimLambertsW <- nimbleFunction(
  run = function(x = double()) {
    REXP <- 2.718281828459045090795598298427648842334747314453125 ## exp(1)
    REXPI <- 0.367879441171442334024277442949824035167694091796875 # exp(-1)
    EPS <- 2.2204460492503131e-16
      
    if (x == Inf) {
      return(Inf)
    } else if (x < -REXPI) {
      return(NaN)
    } else if (abs(x + REXPI) <= EPS) {
      return(-1.0)
    } else if (abs(x) <= 1e-16) {
      return(x)
    } else {
      if (abs(x) <= 6.4e-3) {
        ## When this close to 0 the Fritsch iteration may underflow. Instead,
        ## function will use degree-6 minimax polynomial approximation of Halley
        ## iteration-based values. Should be more accurate by three orders of
        ## magnitude than Fritsch's equation (5) in this range.
        ans <- (((((-1.0805085529250425e1 * x + 5.2100070265741278) * x -
               2.6666665063383532) * x + 1.4999999657268301) * x -
               1.0000000000016802) * x + 1.0000000000001752) * x +
               2.6020852139652106e-18       
        ## Minimax Approximation calculated using R package minimaxApprox 0.1.0
        return(ans);

      } else if (x <= REXP) {
        p = sqrt(2.0 * (REXP * x + 1.0));
        Numer = (0.2787037037037037 * p + 0.311111111111111) * p - 1.0;
        Denom = (0.0768518518518518 * p + 0.688888888888889) * p + 1.0;
        w = Numer / Denom;
      } else {
        w = log(x)
        L_2 = log(w);
        L_3 = L_2 / w;
        L_3_sq = L_3 * L_3;
        w <- w + -L_2 + L_3 + 0.5 * L_3_sq - L_3 / w + L_3 / (w * w) - 
             1.5 * L_3_sq / w + L_3_sq * L_3 / 3.0;
      }
      ## Fritsch Iteration for up to 5 iterations.
      MaxEval <- 5
      CONVERGED <- FALSE
      k <- 2.0 / 3.0;
      i <- 0;
      while (!CONVERGED & i < MaxEval){
        z <- log(x / w) - w
        w1 <- w + 1.0
        q <- 2.0 * w1 * (w1 + k * z)
        qmz <- q - z
        e <- z / w1 * qmz / (qmz - z)
        CONVERGED <- abs(e) <= EPS
        w <- w*(1.0 + e)
        i <- i + 1
      }
      return(w)
    }
    returnType(double())
  }
)
```

# IWAM Model Description and Comparison

## Introduction

The purpose of this report is to:

-   Clearly and simply explain the background, usage, and differences of the TMB and NIMBLE IWAM models
-   Justify the usage of the new Nimble model
-   Introduce next steps in model development
-   Walk through the outputs of both models



## Datamap

The following function vignette/example is from the IWAM Package Repo hosted publicly through the Pacific Salmon Assess organization available at [this link.](https://github.com/Pacific-salmon-assess/iwam-package)

The package currently utilizes the following data sets:

-   *SRinputfile (Private)*: Private data-sets developed by Parken et al. (2006). Required to run the complete function and is available upon request. To be updated with publicly available data. Life histories are included: 1 = ocean type, 0 = stream type. Contains stock names, year, spawners, recruits, and life history identification.
-   *WatershedArea (Public)*: Internal input containing watershed areas per stock.
-   *CUPars_nBC (Public)*: Input file with Ricker $\alpha$'s without bias correction (CITATION) from Diana Dobson's Run Reconstruction coded in TMB. Has a higher estimate of Ricker $\alpha$ (lower Sgen).
-   *CUPars_wBC (Public)*: Input file with Ricker $\alpha$'s with bias correction (CITATION) from Diana Dobson's Run Reconstruction coded in TMB. Has a higher estimate of Ricker $\alpha$ (lower Sgen).
-   *WCVIStocks (Public)*: User watershed area input file. Must contain watershed areas, life history identification, enhancement identification, and stock name. Aggregation within the IWAM function can be done with both CU and Inlet scales.



## Over-arching Model Flow

The model, regardless of its form between Parken et al., Leirmann et al., and Holt et al., follow the following three model stages:

1.  Estimation of the Ricker model using 25 Chinook spawner and recruit time-series from across the NE Pacific

2.  Regression of benchmarks derived from Ricker model parameters against the log(watershed areas) of the stocks included in the synoptic SR time series

3.  Using the regression model, estimate *new* Ricker model parameters and benchmarks for stocks where only watershed area is known

The main differences between the models occur in the for formulation of the Ricker model, and how error is propagated between steps. These differences will be explored in the following document.



## Model Variants

To describe the two main forms of the IWAM model, we will go step by step through the previously described steps, beginning with the core stock-recruitment model, the Ricker model.
<!--CH: suggest relabelling the two model forms: IWAM-Ricker(Smax) and IWAM-Ricker(SREP). They are both integrated, the main difference being how Ricker is paramaterized based on 1/SMAX (which is somewhat, but not entirely standard in salmon) and one based on SREP (which Liermann uses, though this is not well known). And then using consistently through out. Probably best not to label TMB and Nimble (though we should definitely mention the different languages) as that will change soon when they are both transferred to rTMB.-->
-   Clearly and simply explain the background and usage of
    -   the IWAM TMB model 
        -   Based on the Ricker model parameterizd from $\beta$ (or 1/$S_{MAX}$), as in Parken et al. (2006) and Holt et al. 2023 .
    -   the IWAM Nimble model
        -   Based n the Ricker model parameterized from $S_{REP}$, as in  Liermann et al. (2010), current implementation within the NIMBLE model by Paul and myself.
    -   Identify the core differences between the two model variants
        -   The parameterization of the Ricker model  with  $\beta$ (or 1/$S_{MAX}$) vs $S_{REP}$
        -   Methods of estimation (currently TMB vs Nimble)
        -   Parameterized to estimate both benchmarks $S_{REP}$ and $S_{SMSY}$, or only $S_{REP}$
        -   Both models variants are statistically integrated, in contrast to Parken et al. (2006) which is sequential



### SMAX Model Type

Following Parken et al. 2006 and Holt et al. 2023 the SMAX Ricker model is predominantly expressed in the following form:
<!--CH: suggest putting alpha in the exponent here and call it alpha, since in the TMB code we estimate 'loga' which is the a parameter in the exponent, and it would be easier to  align wiht Leirman's r parameter which is also in the exponent. Then remove the log from the linearized form below, and heirachcal prior eqn. -->

$$
R = S e^{log(\alpha)- \beta S}e^\tau
$$

Where $R$ is the abundance of adult recruits expressed as a $numeric$,

$S$ is  spawner abundance expressed as a $numeric$. Is it standard that both recruits and spawners be scaled values for modelling. In the case of the IWAM package, both recruits and spawners are transformed by divided by $10^x$ where $x$ is the maximum number of digits across the per-stock timeseries.

$\alpha$ is the dimensionless number of recruits per spawner produced at  low spawner density (productivity), this may also be thought of as the intercept of the relationship between $\frac{R}{S}$ and $S$,

$\beta$ is the strength of the density dependence expressed as $1/S_{MAX}$ $spawners^{-1}$, which can be described as the slope of the previously described relationship,

and $\tau$ represents normally distributed random error which accounts for process and environmental variability.
<!--TK: This needs a statement for exclusion from the final IWAM model.-->

This is the form used in both the Parken et al. (2006) and the Holt et al. (2023) models.

This model can be linearized for ease of estimation. Further, following Liermann et al. (2010), hierarchical structure is added to the productivity parameter to improve model fit for data-limited populations:

$$
log(\frac{R_i}{S_i}) = log(\alpha)-\beta \cdot S_i - \frac{\sigma_\tau^2}{2}
$$

<!-- Where $\tau \sim Normal(-\sigma_\tau^2/2, \sigma_\tau)$, -->
<!--CH : see line 123 of IWAM_Liermann.cpp-->

Where $i$ is the year class per stock,
<!--TK: There was a form where both \alpha and \beta had _i subscripts. 
Where are those from?-->

and the mean is adjusted by $\frac{-\sigma_\tau^2}{2}$ to account for the log back-transformation adjustment with the following distribution:
<!--TK: Define distribution used and associated justification of choice.-->

$$
\sigma_{\alpha_i} \sim InvGamma(0.01,0.01)
$$

$\alpha$ is the intercept and $\beta$ is the slope of the spawner-recruit relationship, where $log(\alpha)$ has a hierarchical prior:
<!--TK: Define distribution used and associated justification of choice.-->

$$
log(\alpha_i) \sim Normal(\mu_\alpha, \sigma_\alpha)
$$

$$
\mu_{\alpha_i} \sim Normal(1.5,5)
$$

<!--CH: I think we can exclude this given my addition above: Parken et al. (2006) include a log-transformation bias adjustment to productivity estimate $+\frac{\sigma^2}{2}$. -->

Once the Ricker model parameters have been estimated and used to calculate $S_{MSY}$ and $S_{REP}$, the following regression model is estimated:

<!-- To move elsewhere below: Both Parken and Holt form a total of 4 regression models, while Liermann forms two. Between both methods, there are two models - one for stream and one for ocean life histories. The difference is that Liermann only regresses $S_{REP}$ against watershed area, instead of also using $S_{MSY}$ as per Parken. The below equations show the Parken forms: -->

$$
ln(S_{MSY}) = ln(\delta_0) + \delta_1 \cdot ln(WA) + \delta_\epsilon
$$

$$
ln(S_{REP}) = ln(\gamma_o) + \gamma_1 \cdot ln(WA) + \gamma_\epsilon
$$

$$
\delta_\epsilon \sim N(0,\sigma_\delta^2)
$$

$$
\delta_\gamma \sim N(0,\sigma^2_\gamma)
$$

Where $\delta_0$ is the intercept and $\delta_1$ is the slope for the relationship between the natural logarithm of watershed area (WA) and either $S_{MSY}$ and $S_{REP}$. These are then adapted further to separate stream-type and ocean-type fish in the watershed area model. This is done by creating an offset with the baseline as the stream type population. This is an expansion of equations (number). 

$$
ln(S_{MSY}) = ln(\delta_{0,stream}) + ln(\delta_{0,ocean}) \cdot X + (\delta_{1,stream} + \delta_{1,ocean} \cdot X) \cdot ln(WA) + \delta_\epsilon
$$

where, $\delta_\epsilon \sim Normal(0,\sigma_\epsilon)$,

and $X = 0$ for stream type and $X = 1$ is for ocean type populations.

This equation is then repeated to estimate $S_{REP}$ from watershed area.


Once these regression models are estimated, the $\delta$ parameters are used to predict $S_{REP}$ or $S_{MSY}$ for populations where only watershed area is known. When both $S_{REP}$ or $S_{MSY}$ are predicted for a given population (as in Parken et al. 2006), the productivity parameter can be inferred as it is determined exactly from the two benchmarks. This inference of productivity is informed by the underlying spawner-recruitment time-series used to parameterize the model, and can be used to estimate other benchmarks, e.g., $S_{gen}$.

When implementing this model for WCVI Chinook (Holt et al. 2023), the inference of productivity was identified as being implausible (recruits/spawner >7) given low estimates of productivity determined from life-cycle models on these populations (recruits/spawners 2-3). Instead of using watershed-area derived estimate of $S_{MSY}$, Holt et al. (2023) derived bootstrapped distributions of benchmarks $S_{MSY}$ and $S_{gen}$ by drawing from independent distributions of productivity derived either from a life-cycle model or a run-reconstruction and a distribution of $\S_{REP}$ values from the watershed-area model. In future model updates, $\alpha$ could instead be estimated as a regionally hierarchical parameter within the IWAM model and the watershed area regression could use regional groupings as co-variates to predict benchmarks, thereby explicitly accounting for differences in productivities among areas. Alternatively, Liermann et al. (2010) used the formulation of the Ricker based on $S_{REP}$ and predicted only $S_{REP}$ from watershed area, avoiding inferences about productivity.

Here, we follow Holt et al. (2023) to derive bootstrapped uncertainty intervals drawing from distributions for $\alpha$ and the regression model estimates of $S_{REP}$, following these steps:

-   Re-estimate $\beta$ as $\beta = log(\alpha)/S_{REP}$
-   Re-estimate $S_{MSY}$ with the above $\beta$
-   Optimize for an estimate of $S_{GEN}$ using the supplied $\alpha$ and new $\beta$ (See Benchmark estimation for complete equation)

With the optimization complete, we are now supplied with our final estimates of $S_{GEN}$, $S_{MSY}$, and $S_{REP}$.

<!-- Tor: Question: Why is SMSY re-estimated a total of three times? Would the value change a little each time? Does it matter?
CH: My thinking is that we don't use the watershed-area estimate of SMSY because it makes improper inferences about productivity. Instead we use the bootstrapped distbution of SMSY based on alpha and SREP-->



### SREP Model Type
<!--CH: I've changed this to focus on the parameterization of SREP vs b (1/SMAX). Also, TMB estimates loga diectly, which is the same as r in Liermann, so I have removed that difference.-->
The following model is based off of Liermann et al. 2010. The main difference between this formulation of the Ricker model and the one used by Parken et al. (2006) and Holt et al. (2023) is parameterization of $S_{REP}$ instead of $\beta$ (1/$S_{MAX}$). <!--CH suggest removing:  is the scale of $\alpha$. Being either on the real or log-scale, and the parameterization of the watershed area regression to $S_{rep}$. The implication of these differences will be discussed further in the model differences section.-->

<!--CH: I suggest keeping the model parameters as in the equation, and not introduction r and E, this adds confusion as r is equivalent to alpha and SREP to E-->

$$
R_{ij}=S_{ij}e^{log(\alpha_j)(1-\frac{S}{S_{REP_{ij}}})+w_{ij}}
$$

where $i$ is year class,

$j$ is population,

where $R$ is the number of recruits,

$S$ is the number of spawners,

and, $S_{REP}$ is the point on the curve where $R = S$, also defined as the unfished equilibrium point, $S_{REP} = \frac{log\alpha}{\beta}$.

$\alpha_j$ is the $log(slope)$ where $S_{ij} = 0$, with the following prior of:

$$
\alpha_j \sim N(\alpha_m, h_j)
$$

$$
\alpha_m \sim N(0.6, 0.45)
$$

$$
h_j \sim N(0, E_{SD})
$$

$$
E_{SD} \sim Uniform(0,100)
$$

and, $w_{ij}$ is the normally distributed independent random variable with mean 0 and population specific standard deviation.
<!--TK: Is this actually used in the NIMBLE model?-->

To avoid confusion, please keep in mind that Liermann substitutes $\alpha$ for $r$ as a the productivity parameter. We have opted to maintain the nomenclature of $\alpha$ as per the original Ricker equation. This also goes for the reference to $S_{REP}$, which you will see is renamed as $E$ in the Liermann notation. All nomenclature across the model types has been made as close as possible to aid in comparison and evaluation.

This is evaluated as:

$$
log(\frac{R}{S}) \sim N(log(\frac{R}{S}_{pred}), \tau_{obs})
$$

where $\tau$ is the stock-specific precision defined as,

$$
\tau_{obs} \sim Gamma(0.001, 0.001)
$$

With the Ricker parameters defined, the regression is then evluated based on $E$. 
<!--CH: I see this prior on tau is different than for the IWAM-SMAX version of the model. why?-->
<!--TK: We don't define a Tau prior in the IWAM-SMAX version?-->

$$
log(S_{REP_j}) =  b0_{stream,ocean} + bWA_{stream,ocean}*log(WA_j) + log(E0_j)
$$
<!--CH: I'm not sure why the lob(E0_j) term is needed. Why this instead of a random normal deviate as in  Liermann et al. - looking at their eqn 2. In preditions, this is set to zero, correct?-->

where $b0$ is the intercept (either river or ocean),

$$
b0 \sim N(0, \tau = 0.001)
$$

$bWA$ is the slope (either river or ocean),

$$
bWA \sim N(0, \tau = 0.001)
$$

$WA$ is the watershed area $(km^2)$ for the population,

and $E0$ is the stock-level random-effect, with the following prior:

$$
log(E0_j) \sim N(0, log(E_{SD}))
$$

$$
log(E_{SD}) \sim Uniform(0, 100)
$$

Armed now with the regression coefficients $b0$ and $bWA$, we may estimate new stock $E$'s (keep in mind this is $S_{REP}$) and assuming the same global $\alpha$ (or $r$) estimate $S_{MSY}$ and $S_{GEN}$.



## Benchmark Estimation

Using the Ricker estimated $\alpha$ and $\beta$ (from the Parken/Holt model type) or $E$ (from the Liermann model type), the following equations are used for $S_{MSY}$ and $S_{REP}$.

$$
S_{REP} = \frac{log(\alpha)}{\beta}
$$

The solution to $S_{MSY}$ is solved with a Lambert W equation defined by Schueurel (2016).

$$
S_{MSY} = \frac{1-LambertW(e^{1-(\alpha)})}{\beta}
$$

The same equations are used for these calculations regardless of model. The only two exceptions are that we use an explicit form of LambertW re-created in framework for Nimble, and $S_{REP}$ is explicitly already calculated in the Liermann method.

$S_{MSY}$, $\beta$, and $\alpha$ are then used in the final calculation of $S_{GEN}$, shown explicity here as:

$$
S_{GEN} = -1/\beta*LambertW(-\beta*\frac{S_{MSY}}{\alpha})
$$



## Comparison of Model Steps and Main Model Differences
<!-- Tor: how am I going to show the differences?-->

-   Identify the core differences between the two model variants
    -   The parameterization of $S_{REP}$, and by that measure the exclusion of $\beta$
    -   Methods of estimation: TMB versus Nimble
    -   Propagation of error: sequential versus statistical integration approaches

The core difference between the two model types stems from the difference in the parameterization of the watershed are regression model. In the first approach applied in Parken et al. (2006), regression models are estimated for $S_{REP}$ and $S_{MSY}$, whereas the Liermann et al. (2010) only estimate a regression model for $S_{REP}$ and uses a version of the Ricker model that estimates this directly. While these model forms often given similar parameter values, Liermann et al. (2010) argues that isolating for $S_{REP}$ is simpler,  more biologically interpretable, and is often required for assessment and management purposes.  <!--CH: not sure what this means, suggest omitting: We can ask the question, what would the difference between the regressions of $S_{REP}$ and $S_{MSY}$ be, given that they are both estimated from the same $\alpha$ and $\beta$.-->



## Justification for continued usage of the $S_{REP}$ model

-   Justify the usage of the new Nimble model and how to choose between them:
    -   The Liermann method is a direct method of estimation of $S_{REP}$
    -   The Holt method is a direct estimation of $S_{MSY}$ or $S_{MAX}$
    -   Model implementation in the future e.g. design matrix formatting for the inclusion of a) more model parameters, and b) varying model structures
    -   What does the client want in terms of benchmarks?
    


## Next Steps

-   Next steps in model development
    -   Model analysis and investigation (posterior predictive checks)
    -   Consideration and discussion around $S_{REP}$ and $S_{MAX}$ as management benchmarks. This would include a potential re-parameterization of the model for $S_{MAX}$
    -   Translations of both models into RTMB
    -   Incorporation of a regionally hierarchical $\alpha$ parameter
    -   Consider Parken's implementation of an internal productivity covariate
    -   Consider implementation of the Liermann random-walk $\alpha$
    -   Development of packages/functions for ease of use by managers



## Model Usage

The following briefly explains the usag eof the IWAM integrated function (for the TMB version of the Holt et al. 2023) model, and subsequently the NIMBLE model example code.

### TMB Model Inputs and Run

<!-- Tor: insert IWAM model full setup and run -->

The TMB model for the Holt implementation can be accessed through a single function: `IWAM_func` from the IWAM repository of Pacific-Salmon-Assess. The function requires only the input of stocks and watersheds for those that you wish to estimate stock recruitment relationships. In the case below: `WCVIStocks.csv`. The remaining function defaults are to control the presence or absense of enhancement, and the parameters of the bootstrapping.

```{r iwam and parken model setup, message=FALSE, warning=FALSE, results='hide'}
IWAM_WCVI_noEnh <- IWAM_func(WAin = "DataIn/WCVIStocks.csv", # Input data
                       remove.EnhStocks = TRUE, # Enhancement input
                       run.bootstrap = TRUE, # On/off for bootstrap process
                       bs_seed = 1, # internal seed for the bootstrap
                       bs_nBS = 10, # internal number of trials for the bootstrap (default 20000)
                       plot = FALSE, # print plots to 'DataOut/'
                       est.table = FALSE # store kable formatted tables
)
```

```{r tmb ouputs, echo=FALSE, message=FALSE, warning=FALSE}
SMSY_tmb <- IWAM_WCVI_noEnh[[3]] %>%
  filter(RP=='SMSY')
SREP_tmb <-IWAM_WCVI_noEnh[[3]] %>%
  filter(RP=='SREP')

Parken_WCVI <- read.csv(here::here("DataIn/WCVI_Parken.csv"))
  # Could just use WCVI_Parken from the beginning here - then I don't have to do this join below
Parken_est <- read.csv(here::here("DataIn/Parkenest.csv"))

eval_dat <- Parken_WCVI %>% left_join(SMSY_tmb, by=join_by(Stock)) %>% 
  rename("SMSY" = Value, "UL SMSY" = upr, "LL SMSY" = lwr) %>%
  select(-RP) %>% 
  left_join(SREP_tmb, by=join_by(Stock)) %>%
  rename("SREP" = Value, "UL SREP" = upr, "LL SREP" = lwr) %>%
  select(-RP) %>% 
  mutate(PA_UL_SMSY = PA_SMSY + (1.96 * PA_SE_SMSY)) %>% 
  mutate(PA_LL_SMSY = PA_SMSY - (1.96 * PA_SE_SMSY)) %>% 
  mutate(PA_UL_SREP = PA_SREP + (1.96 * PA_SE_SREP)) %>% 
  mutate(PA_LL_SREP = PA_SREP - (1.96 * PA_SE_SREP))

# Standard residuals
SRes_tmb <- IWAM_WCVI_noEnh$SRes
srdat2 <- IWAM_WCVI_noEnh$srdat

mod <- "IWAM_Liermann" 

lifehist <- srdat2 %>% dplyr::select(Stocknumber, Name, Stream) %>% # should this be srdat or srdat2?
  group_by(Stocknumber) %>% 
  summarize(lh=max(Stream)) %>% 
  arrange (Stocknumber)

# pred_lnWA <- seq(min(log(WAbase$WA)), max(log(WAbase$WA)), 0.1)
```



### NIMBLE Model Inputs and Run

<!-- Tor: Insert Paul's new Nimble model with full run -->

```{r nimble model setup, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Setup data:
srdatwna <- read.csv(here::here("DataIn/SRinputfile.csv"))
WAbase <- read.csv(here::here("DataIn/WatershedArea.csv"))

srdatwna <- srdatwna %>% 
  filter(!Name %in% c("Hoko","Hoh")) 
  
# Remove years with NAs and renumber.
srdat <- srdatwna %>% 
  filter(!Name %in% c("Hoko","Hoh")) %>% 
  filter(Rec != "NA") %>%
  filter( !(Name == "Cowichan" & (Yr < 1985 | Yr == 1986 | Yr == 1978))) %>%
  group_by(Name, Stocknumber, Stream) %>%
  arrange(Yr) %>%
  mutate(yr_num = 0:(n()-1)) %>%
  ungroup() %>%
  arrange(Stocknumber) %>%
  mutate(lh = factor(ifelse(Stream == 0, "stream", "ocean"), levels = c("stream", "ocean")))

names <- srdat %>% 
  dplyr::select (Stocknumber, Name, lh) %>% 
    distinct()

WAbase <- WAbase %>% 
  full_join(names, by="Name") %>% 
    arrange(Stocknumber) %>%
    mutate(logWA = log(WA)) 

## Shift log WA for the mean.
mean_logWA <- mean(WAbase$logWA)
WAbase$logWAshifted <- WAbase$logWA - mean_logWA

data <- list(
  logRS = log(srdat$Rec) - log(srdat$Sp)  
)

constants <- list()
constants$N_Stk = max(srdat$Stocknumber + 1)
constants$stk = srdat$Stocknumber + 1
constants$N_Obs = nrow(srdat)
constants$S = srdat$Sp
constants$betaPriorMean = c(10,10,0,0)
# constants$betaPriorMean = c(10, 0,0,0)

## Build a linear model:
# constants$X <- model.matrix( ~ -1+lh+lh:logWAshifted, data = WAbase)
constants$X <- model.matrix( ~ lh*logWAshifted, data = WAbase)
constants$nbeta <- ncol(constants$X)

inits <- function(){
  list(beta = c(10,0,0,0),
  # list(beta = c(10,10,0,0),
    logAlpha0 = 1.5,
    logESD = 1,
    logAlphaSD = 1)
}
```

```{r nimble model core, message=FALSE, warning=FALSE, results='hide'}
stock_recruit_srep_biasCor <- nimbleCode({
  ## priors
  logAlpha0 ~ dnorm(mean=0.6, sd=0.45)
  # logAlpha0 ~ dnorm(mean=1.5, sd=2.5)
  logAlphaSD ~ dunif(0, 100)
  logESD ~ dunif(0, 100)
  ## Can we create regional mean alpha's?
  ## Can we use the distribution's used by Diane Dobson - see bootstrap

  ## Based on Parken et al. 2006 (log ( 1/beta ) has a linear relationship with log Watershed Area.
  for(i in 1:nbeta) beta[i] ~ dnorm(betaPriorMean[i], tau = 0.001)

  for( i in 1:N_Stk ) { # 25 iterations - number of base stocks
    logAlpha[i] ~ dnorm(logAlpha0, sd = logAlphaSD) ## Random effect for log alpha.
    logE0[i] ~ dnorm(mean = 0, sd = logESD) ## Stock level random effect - rename to ij (normally distributed)
      # or a small greek variable

    log(E[i]) <- inprod(beta[1:nbeta], X[i,1:nbeta]) + logE0[i] ## Stock level regression
    tauobs[i] ~ dgamma(0.001, 0.001) # stock specific precision
  }

  ## Model and residuals:  
  for(i in 1:N_Obs){ # 501 iterations - number of base observations
    logRS_pred[i] <- logAlpha[stk[i]]*(1 - S[i]/E[stk[i]])
    logRS[i] ~ dnorm( logRS_pred[i], tau = tauobs[stk[i]]) # stock specific precision - change to be variance - does that affect the code?
  }
})
```

```{r nimble run, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
model <- nimbleModel(stock_recruit_srep_biasCor, data = data, constants = constants,
  inits = inits(), buildDerivs=FALSE) # Build derivs if you want MLE
conf <- configureMCMC(model)
conf$setMonitors(c("logAlpha", "logAlpha0", "E", "logRS_pred",
                    "beta", "tauobs",
                    "logAlphaSD", "logESD", "logE0"))
mcmc <- buildMCMC(conf)
cmodel <- compileNimble(model)

cmcmc <- compileNimble(mcmc, project = model)
mcmc.out <- runMCMC(cmcmc, niter=50000, nburnin=5000, nchains=3, samplesAsCodaMCMC = TRUE)

beta.out <- do.call('rbind', mcmc.out[, grepl("beta\\[", colnames(mcmc.out[[1]]))])
slopes <- summary(mcmc(beta.out))
# MCMCtrace(object = mcmc.out, params = "beta")

beta.out <- do.call('rbind', mcmc.out[, grepl("beta\\[", colnames(mcmc.out[[1]]))])
slopes <- summary(mcmc(beta.out))
```

```{r nimble pp, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
## This is good to check how well your model works:
posteriorPredictiveFunction <- nimbleFunction(
  setup = function(model, mcmc){
    dataNodes <- model$getNodeNames(dataOnly = TRUE)
    parentNodes <- model$getParents(dataNodes, stochOnly = TRUE)
    cat("Stochastic parents of data are:", paste(parentNodes, collapse = ','), ".\n")
    simNodes <- model$getDependencies(parentNodes, self = FALSE)
    vars <- mcmc$mvSamples$getVarNames()  # need ordering of variables in mvSamples / samples matrix
    varsScalar <- model$expandNodeNames(vars, returnScalarComponents = TRUE)
    cat("Using posterior samples of:", paste(vars, collapse = ','), ".\n")
    n <- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))
    samples <- matrix(0, nrow = 2, ncol = 2)
    nMCMC <- 2
    Smsy <- matrix(0, nrow = 2, ncol = 2)
    Sgen <- matrix(0, nrow = 2, ncol = 2)
    Srep <- matrix(0, nrow = 2, ncol = 2)
    Smax <- matrix(0, nrow = 2, ncol = 2)
  },
  run = function(){},
  methods = list(
    getVarNames = function(){
      returnType(character(1))
      return(varsScalar)
    },
    posteriorPredict = function(mcmcSamples = double(2)){
      samples <<- mcmcSamples
      nSamp <- dim(samples)[1]
      ppSamples <- matrix(nrow = nSamp, ncol = n)
      for(i in 1:nSamp) {
            values(model, vars) <<- samples[i, ]
            model$simulate(simNodes, includeData = TRUE)
            ppSamples[i, ] <- values(model, dataNodes)
      }
      returnType(double(2))
      return(ppSamples)
    },
    saveMCMC = function(mcmcSamples = double(2)){
      samples <<- mcmcSamples   
      nMCMC <<- dim(samples)[1]
    },
    ## Assumes X is sorted by stk when passed at obsLevel.
    calcRefPoints = function(X = double(2), obsLevel = integer(0, default = 0)){
      npred <- dim(X)[1]
      Smsy <<- matrix(0, nrow = nMCMC, ncol = npred)
      Sgen <<- matrix(0, nrow = nMCMC, ncol = npred)
      Srep <<- matrix(0, nrow = nMCMC, ncol = npred)
      Smax <<- matrix(0, nrow = nMCMC, ncol = npred)
      
      if(obsLevel == 1 & npred != dim(model$logAlpha)[1]){
        stop("Can only predict for the observed locations")
      }
      for( i in 1:nMCMC ){
      values(model, vars) <<- samples[i, ]
        for( j in 1:npred){
          if(obsLevel == 0 ) logalpha <- rnorm(1, mean = model$logAlpha0[1], sd = model$logAlphaSD[1])
          else logalpha <- model$logAlpha[j]
          E <- exp(inprod(values(model, "beta"), X[j,]) + obsLevel*model$logE0[j])
          beta <- logalpha/E
          Smax[i,j] <<- 1/beta
          Srep[i,j] <<- E
          Smsy[i,j] <<- (1-nimLambertsW(exp(1-logalpha)))/beta
          Sgen[i,j] <<- -1/beta*nimLambertsW(-beta*Smsy[i,j]/(exp(logalpha)))
        }
      }
    },
    getRefPostPred = function(type = character(0, default = "Smsy")){
      returnType(double(2))
      if(type == "Smsy") return(Smsy)  
      if(type == "Sgen") return(Sgen)
      if(type == "Smax") return(Smax)
      if(type == "Srep") return(Srep)
      print("Returning Srep")
      return(Srep)
    }
  )
)

```

#### .  Nimble Ouputs (for coding purposes only - delete on knit)

```{r nimble outputs, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
predict <- posteriorPredictiveFunction(model, mcmc)
cpredict <- compileNimble(predict)


summarize_all_ref_pts <- function(predInfo){
  summarize <- function(mat){
    res <- data.frame()
    for( i in 1:ncol(mat) ){
      res <- rbind(res, data.frame(Mean = mean(mat[,i]), SD = sd(mat[,i]), 
        LowerCI = quantile(mat[,i], 0.025), Median = quantile(mat[,i], 0.5), UpperCI = quantile(mat[,i], 0.975), 
        row.names = i)
      )
    }
    return(res)
  }
  out <- data.frame()
  for( i in c("Srep", "Smax", "Smsy", "Sgen") ){
    tmp <- summarize(cpredict$getRefPostPred(i))
    tmp$type <- i
    tmp <- cbind(predInfo, tmp)
    out <- rbind(out, tmp)
  }  
  return(out)
}


## Prediction design matrix:
WAin <- read.csv(here::here("DataIn/WCVIStocks.csv"))
WAin$logWA <- log(WAin$WA)
WAin$logWAshifted <- WAin$logWA-mean_logWA
WAin <- WAin %>%
  mutate(lh = factor(ifelse(lh == 0, "stream", "ocean"), levels = c("stream", "ocean")))
Xpred <- model.matrix( ~ lh*logWAshifted, data = WAin)  ## This has to match model input.
# Xpred <- model.matrix( ~ -1+lh+lh:logWAshifted, data = WAin)
## Make sure internal variable names match with output.
getOrder <- cpredict$getVarNames()

## Predict new sites:
samples <- do.call("rbind", mcmc.out)[, getOrder] ## If this fails need to check your MCMC tracked vars.
cpredict$saveMCMC(samples)
cpredict$calcRefPoints(Xpred) ## 45,000 x 5 this is a big operation.
sum.pred <- summarize_all_ref_pts(WAin)

## Make a prediction matrix for a line:
WAline <- expand.grid(lh = c("stream", "ocean"), logWA = 2:14)
WAline <- WAline %>%
  mutate(lh = factor(lh, levels = c("stream", "ocean")),
    logWAshifted = logWA - mean_logWA)
Xline <- model.matrix( ~ lh*logWAshifted, data = WAline) ## This has to match model input.
# Xline <- model.matrix( ~ -1+lh+lh:logWAshifted, data = WAline)
cpredict$calcRefPoints(Xline) ## 45,000 x 5 this is a big operation.
sum.line <- summarize_all_ref_pts(WAline)
  
## Make a prediction matrix for observations
Xbase <- model.matrix( ~ lh*logWAshifted, data = WAbase) ## This has to match model input.
# Xbase <- model.matrix( ~ -1+lh+lh:logWAshifted, data = WAbase)
cpredict$calcRefPoints(Xbase, 1) ## 45,000 x 5 this is a big operation.
sum.base <- summarize_all_ref_pts(WAbase)
```

```{r old nimble outputs, eval=FALSE, echo=FALSE}
## Prediction design matrix:
WAin <- read.csv(here::here("DataIn/WCVIStocks.csv"))
WAin$logWA <- log(WAin$WA)
WAin$logWAshifted <- WAin$logWA-mean_logWA
WAin <- WAin %>%
  mutate(lh = factor(ifelse(lh == 0, "stream", "ocean"), levels = c("stream", "ocean")))
Xpred <- model.matrix( ~ lh*logWAshifted, data = WAin)  ## This has to match model input.

## Make sure internal variable names match with output.
getOrder <- cpredict$getVarNames()

## Predict new sites:
Smsy <- Sgen <- Srep <- NULL
for( i in 1:length(mcmc.out)){
  samples <- mcmc.out[[i]][, getOrder] ## If this fails need to check your MCMC tracked vars.
  cpredict$saveMCMC(samples)
  cpredict$calcRefPoints(Xpred)
  Smsy <- rbind(Smsy, cpredict$getSmsy())
  Sgen <- rbind(Sgen, cpredict$getSgen())
  Srep <- rbind(Srep, cpredict$getSrep())
}

sum.smsy <- do.call("cbind", summary(mcmc(Smsy)))
colnames(sum.smsy) <- paste0("Smsy_",colnames(sum.smsy))
sum.sgen <- do.call("cbind", summary(mcmc(Sgen)))
colnames(sum.sgen) <- paste0("Sgen_",colnames(sum.sgen))
sum.srep <- do.call("cbind", summary(mcmc(Srep)))
colnames(sum.srep) <- paste0("Srep_",colnames(sum.srep))

## 
WAin <- WAin %>% 
  bind_cols(sum.smsy) %>% 
  bind_cols(sum.sgen) %>% 
  bind_cols(sum.srep)

## Make a prediction matrix for a line:
WAline <- expand.grid(lh = c("stream", "ocean"), logWA = 2:14)
WAline <- WAline %>%
  mutate(lh = factor(lh, levels = c("stream", "ocean")),
    logWAshifted = logWA - mean_logWA)
Xline <- model.matrix( ~ lh*logWAshifted, data = WAline) ## This has to match model input.
  
## Find the posterior predictions on the line:
Smsy_line <- Sgen_line <- Srep_line <- NULL
for( i in 1:length(mcmc.out)){
  samples <- mcmc.out[[i]]
  samples <- samples[,order(colnames(samples))]
  cpredict$saveMCMC(samples)
  cpredict$calcRefPoints(Xline) ## Design matrix for this prediction.
  Smsy_line <- rbind(Smsy_line, cpredict$getSmsy())
  Sgen_line <- rbind(Sgen_line, cpredict$getSgen())
  Srep_line <- rbind(Srep_line, cpredict$getSrep())
}

sum.smsy.line <- do.call("cbind", summary(mcmc(Smsy_line)))
colnames(sum.smsy.line) <- paste0("Smsy_",colnames(sum.smsy.line))
sum.sgen.line <- do.call("cbind", summary(mcmc(Sgen_line)))
colnames(sum.sgen.line) <- paste0("Sgen_",colnames(sum.sgen.line))
sum.srep.line <- do.call("cbind", summary(mcmc(Srep_line)))
colnames(sum.srep.line) <- paste0("Srep_",colnames(sum.srep.line))

## 
WAline <- WAline %>% 
  bind_cols(sum.smsy.line) %>% 
  bind_cols(sum.sgen.line) %>% 
  bind_cols(sum.srep.line)


## Make a prediction matrix for observations
Xbase <- model.matrix( ~ lh*logWAshifted, data = WAbase) ## This has to match model input.
  
## Find the posterior predictions on the line:
Smsy_base <- Sgen_base <- Srep_base <- NULL
for( i in 1:length(mcmc.out)){
  samples <- mcmc.out[[i]]
  samples <- samples[,order(colnames(samples))]
  cpredict$saveMCMC(samples)
  cpredict$calcRefPoints(Xbase, obsLevel = 1) ## Design matrix for this prediction.
  Smsy_base <- rbind(Smsy_base, cpredict$getSmsy())
  Sgen_base <- rbind(Sgen_base, cpredict$getSgen())
  Srep_base <- rbind(Srep_base, cpredict$getSrep())
}

sum.smsy.base <- do.call("cbind", summary(mcmc(Smsy_base)))
colnames(sum.smsy.base) <- paste0("Smsy_",colnames(sum.smsy.base))
sum.sgen.base <- do.call("cbind", summary(mcmc(Sgen_base)))
colnames(sum.sgen.base) <- paste0("Sgen_",colnames(sum.sgen.base))
sum.srep.base <- do.call("cbind", summary(mcmc(Srep_base)))
colnames(sum.srep.base) <- paste0("Srep_",colnames(sum.srep.base))

## 
WAbase <- WAbase %>% 
  bind_cols(sum.smsy.base) %>% 
  bind_cols(sum.sgen.base) %>% 
  bind_cols(sum.srep.base)

```

```{r nimble residuals setup, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
## Posterior Predictive Checks:
## Combine random amounts of the multiple chains.
samples <- NULL
for( i in 1:length(mcmc.out) ){
  samples <- rbind(samples, mcmc.out[[i]][sample(nrow(mcmc.out[[1]]), ceiling(0.25*nrow(mcmc.out[[1]]))), getOrder])
}
ppsim <- cpredict$posteriorPredict(samples)
logRS_predMean <- apply(samples[,grep("logRS_pred", colnames(samples)) ], 2, mean)
DHARMaRes <- createDHARMa(simulatedResponse = t(ppsim), observedResponse = data$logRS, 
             fittedPredictedResponse = logRS_predMean, integerResponse = FALSE)
```

```{r residual plots extra, echo=FALSE}
# plot(DHARMaRes)
# 
# boxplot(DHARMaRes$scaledResiduals~srdat$Name)
# abline(h = 0.5, col = 'red')
# 
srdat$Res <- DHARMaRes$scaledResiduals
# ggplot(data = srdat, aes(x = Name, y = res)) +
#   geom_boxplot() +
#   geom_hline(yintercept = 0.5, col = 'red') + 
#   theme_classic()
# 
logalpha.sum <- summary(mcmc.out[, grep('logAlpha\\[', colnames(mcmc.out[[1]]))])
srdat.alpha <- WAbase %>% bind_cols(data.frame(do.call("cbind", logalpha.sum)))
# ggplot(data = srdat.alpha, aes(x = Name, y = Mean)) +
#   geom_point() +
#   geom_errorbar(aes(ymin = X2.5., ymax = X97.5.)) +
#   theme_classic()
```

### Comparison of Ouputs

#### Spawner-recruit curves {.tabset}

##### TMB {.tabset}

```{r stock-recruit curve code, include=FALSE}
# PlotSRLinear(srdat=srdat, pars=pars, r2=r2, removeSkagit = FALSE)
# PlotSRCurve(srdat=srdat, pars=pars, r2=r2, removeSkagit = FALSE, mod=mod)
```

###### Linear
```{r tmb sr linear, echo=FALSE, warning=FALSE, message=FALSE}
# what is r2
PlotSRLinear(srdat=srdat2, 
             pars=IWAM_WCVI_noEnh$modelpars, 
             r2=IWAM_WCVI_noEnh$r2, 
             removeSkagit = FALSE)

# can i label axes globally for this?
```
###### Curve
```{r tmb sr curve, echo=FALSE, warning=FALSE, message=FALSE}
PlotSRCurve(srdat=srdat2, 
            pars=IWAM_WCVI_noEnh$modelpars, 
            r2=IWAM_WCVI_noEnh$r2, 
            removeSkagit = FALSE,
            mod = mod)

# why is there no legend for this?
# What is the line?
# what is the grey bar?
# what is the secondary dashed line?

# can i label axes globally for this?
```

##### NIMBLE {.tabset}
```{r stock-recruit curve code nimble, include=FALSE}
# PlotSRLinear(srdat=srdat, pars=pars, r2=r2, removeSkagit = FALSE)
# PlotSRCurve(srdat=srdat, pars=pars, r2=r2, removeSkagit = FALSE, mod=mod)

# For comparison call on:
    # pars=IWAM_WCVI_noEnh$modelpars, 
    # r2=IWAM_WCVI_noEnh$r2,
```

###### Linear
```{r nimble sr linear,  echo=FALSE, warning=FALSE, message=FALSE}
# PlotSRLinear(srdat=srdat,
#              pars=pars,
#                 # logB, logA, SMSY NEEDED
#                 # Estimate, Std.error, param, stocknumber, name, lh
#              r2=r2, # correlation matrix?
#                 # r2 <- all_pred %>% group_by(Stocknumber) %>% summarize(r2=cor(ObslogRS,Pred)^2)
#                 # contains: stocknumber and r2 values
#              removeSkagit = FALSE)
```

###### Curve
```{r nimble sr curve,  echo=FALSE, warning=FALSE, message=FALSE}
# PlotSRCurve(srdat=srdat,
#             pars=pars, # NEED
#             r2=r2, # NEED
#             removeSkagit = FALSE,
#             mod=mod)
```

#### Stock-wise Comparisons of Benchmarks {.tabset}

##### SREP

```{r stockwise SREP, echo=FALSE, message=FALSE}
# config options: fig.align = 'centre'
nimble_bench <- sum.pred %>%
  pivot_wider(names_from = type, values_from = c(Mean, SD,LowerCI, Median, UpperCI))

benchmarks <- eval_dat %>% 
  full_join(nimble_bench, by=c("Stock"))
# pivot_wider



benchmarks_srep <- ggplot(benchmarks, aes(x=Stock, y = SREP)) +
  geom_errorbar(aes(ymax = `UL SREP`, ymin = `LL SREP`, color='TMB',), width = 0.2, position = position_nudge(0.2)) +
  geom_point(aes(color = 'TMB'), position = position_nudge(0.2)) +
  
  geom_point(aes(x = Stock, y = PA_SREP, color='Parken'), position = position_nudge(-0.2)) + 
  geom_errorbar(aes(x = Stock, ymax = PA_UL_SREP, ymin = PA_LL_SREP, color='Parken'), width = 0.2, position = position_nudge(-0.2)) +
  
  geom_point(aes(x = Stock, y = Mean_Srep, color='Nimble')) +
  geom_errorbar(aes(x = Stock, ymax = UpperCI_Srep, ymin = LowerCI_Srep, color='Nimble'), width = 0.2, inherit.aes = FALSE) +
  theme_classic() + 
  ylab(TeX("$S_{REP}$ Estimate")) + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.3, hjust = 1)) +
  scale_color_manual(name='Model',
                     breaks=c('Parken', 'TMB', 'Nimble'),
                     values=c('Parken'='black', 'TMB'='red', 'Nimble'='blue'))

benchmarks_srep
```

##### SMSY

```{r stockwise SMSY, echo=FALSE, message=FALSE}
# Init plot and add geom for IWAM estimates as baselines

# combine eval_dat and WAin in order to have one dataset to make the legend
benchmarks_smsy <- ggplot(benchmarks, aes(x=Stock, y = SMSY)) +
  geom_errorbar(aes(ymax = `UL SMSY`, ymin = `LL SMSY`, color='TMB',), width = 0.2, position = position_nudge(0.2)) +
  geom_point(aes(color = 'TMB'), position = position_nudge(0.2)) +
  
  geom_point(aes(x = Stock, y = PA_SMSY, color='Parken'), position = position_nudge(-0.2)) + 
  geom_errorbar(aes(x = Stock, ymax = PA_UL_SMSY, ymin = PA_LL_SMSY, color='Parken'), width = 0.2, position = position_nudge(-0.2)) +
  
  geom_point(aes(x = Stock, y = Mean_Smsy, color='Nimble')) +
  geom_errorbar(aes(x = Stock, ymax = UpperCI_Smsy, ymin = LowerCI_Smsy, color='Nimble'), width = 0.2, inherit.aes = FALSE) +
  theme_classic() + 
  ylab(TeX("$S_{MSY}$ Estimate")) + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.3, hjust = 1)) +
  scale_color_manual(name='Model',
                     breaks=c('Parken', 'TMB', 'Nimble'),
                     values=c('Parken'='black', 'TMB'='red', 'Nimble'='blue'))

benchmarks_smsy
```


#### WA Regression {.tabset}

##### TMB {.tabset}

```{r tmb reg prep, echo=FALSE}
# IWAM_
  # pars: 
# mod <- "IWAM_Liermann" 
# lifehist <- srdat %>% dplyr::select(Stocknumber, Name, Stream) %>% 
#   group_by(Stocknumber) %>% 
#   summarize(lh=max(Stream)) %>% 
#   arrange (Stocknumber)
# pred_lnWA <- seq(min(log(WAbase$WA)), max(log(WAbase$WA)), 0.1)
pred_lnWA <- seq(min(log(WAbase$WA)), max(log(WAbase$WA)), 0.1)
# re-create the correct srdat w/ scale - just copy in the Iwam code for it
# srdat2 <- IWAM_WCVI_noEnh$srdat
```

###### SREP
```{r tmb srep wa reg, echo=FALSE, warning=FALSE, message=FALSE}
plotWAregressionSREP(pars=IWAM_WCVI_noEnh$modelpars, 
                             IWAM_WCVI_noEnh$all_Deltas, 
                             srdat2, 
                             lifehist,
                             WAbase=WAbase, 
                             pred_lnSREP=IWAM_WCVI_noEnh$pred_lnSREP, 
                             pred_lnWA = pred_lnWA, # this is wrong
                             title1 = "SREP Regression",
                             mod=mod)
```

###### SMSY
```{r tmb SMSY wa reg, echo=FALSE, warning=FALSE, message=FALSE}
plotWAregressionSMSY(pars=IWAM_WCVI_noEnh$modelpars, 
                             IWAM_WCVI_noEnh$all_Deltas, 
                             srdat2, 
                             lifehist,
                             WAbase=WAbase, 
                             pred_lnSMSY=IWAM_WCVI_noEnh$pred_lnSMSY, 
                             pred_lnWA = pred_lnWA, # this is wrong
                             title1 = "SMSY Regression",
                             mod=mod)
```

##### NIMBLE

```{r nimble wa reg, echo=FALSE, warning=FALSE, message=FALSE}
# filter(WAline, lh == 'ocean')

# intercept is larger because we shifted the intercept to the right, following Liermann, mean(logWA)
  # everything is fitted the same
  # seemed to improve convergence with Liermann
  # typically scaling technique

# NOW USING: sum.line

ggplot(data = WAline, aes(x = logWA, y = log(`Srep_50%`), colour = lh, fill = lh)) +
  geom_line() + 
  geom_ribbon(aes(ymin = log(`Srep_2.5%`), ymax = log(`Srep_97.5%`)), alpha = .1) +

  geom_point(data = WAin, mapping = aes(logWA, log(`Srep_50%`), colour = lh), alpha=0.2) +
  geom_errorbar(data = WAin, mapping = aes(ymin = log(`Srep_2.5%`), ymax = log(`Srep_97.5%`), colour = lh), alpha=0.2) +

  geom_point(data = WAbase, mapping = aes(logWA, log(`Srep_50%`), colour = lh)) +
  geom_errorbar(data = WAbase, mapping = aes(ymin = log(`Srep_2.5%`), ymax = log(`Srep_97.5%`), colour = lh )) + 
  theme_classic() +
  ylab("Log Srep") +
  xlab("Log Watershed Area") +
  annotate("text", x=10, y=5, label= "8.91 + 0.68*log(WA)") + # stream
  annotate("text", x=5, y=15, label= "10.04 + 0.95*log(WA)") # ocean
```

##### Parken et al. 2006 {.tabset}

###### Figures
![unchanged image](Parkenregressions.png)
<!-- Tor: The comparison of the 3 figures-->

###### Parameter Table
![Caption for the picture.](Parkenregparametertable.png)
<!-- Tor: The table of parameter values-->



#### ACF {.tabset}

##### TMB

```{r tmb acf plot, echo=FALSE, message=FALSE, warning=FALSE}
Plotacf(SRes_tmb)
```

##### NIMBLE

```{r nimble acf, echo=FALSE}
Plotacf(srdat)
```

#### Residuals {.tabset}

##### TMB

```{r tmb stres plot, echo=FALSE, message=FALSE, warning=FALSE}
PlotStdResid(SRes_tmb)
```

##### NIMBLE

```{r nimble residuals, echo=FALSE}
PlotStdResid(srdat)
```

## Appendices

<!-- If needed -->

```{r tmb SREP wa reg OLD, include=FALSE, eval=FALSE}
# IWAM_WCVI_noEnh --> 
  # eval_dat - only includes the point estimates of SREP/SMSY (no SGEN)
  # Need to get model parameters for betas and recreate the line - not the same as predicting the line with the ribbon as the Nimble model - I'm not sure I can do that in the same way - may need to ask Paul
  # IWAM_WCVI_noEnh$all_Deltas has the deltas required to plot the line itself - but I wouldn't get the ribbon from this method
  # SMSY: deta1, delta1_ocean, delta2, delta2_ocean
  # SREP: nu1, nu1_ocean, nu2, nu2_ocean
  # keep in mind that these need to be are added as offsets
# e.g. logNu1 + logNu1_ocean + (exp(logNu2) + Nu2_ocean) * target_lnWA_ocean(i) --> SREP
nu_int <- IWAM_WCVI_noEnh$all_Deltas[5,1] + IWAM_WCVI_noEnh$all_Deltas[6,1]
nu_slope <- exp(IWAM_WCVI_noEnh$all_Deltas[7,1]) + IWAM_WCVI_noEnh$all_Deltas[8,1]
# logDelta1 + logDelta1_ocean + (exp(logDelta2) + Delta2_ocean) * target_lnWA_ocean(i) --> SMSY


# Old plotting function:
# plotWAregressionSMSY (pars, all_Deltas, srdat, lifehist, WAbase, pred_lnSMSY, 
#                           pred_lnWA = data$pred_lnWA, title1=title_plot, mod)
  # old function plots CI's using reg off: pred_lnSREP_stream_CI
  # Not sure this is correct
  # the old function also draws polygons based on the forward and reverse ordering of the log(WA) - feels off
  # these can now be accessed through pred_lnSMSY and pred_lnSREP
nu_errmin <- IWAM_WCVI_noEnh$pred_lnSREP[73:144,2]
nu_errmax <- IWAM_WCVI_noEnh$pred_lnSREP[73:144,2]

ggplot(data = eval_dat, aes(x = log(WA), y = log(SREP), colour = lh, fill = lh)) +
  geom_point() +
  geom_errorbar(aes(ymin = log(`LL SREP`), ymax = log(`UL SREP`))) +
  geom_abline(intercept = nu_int,
              slope = nu_slope,
              color = 'black') +
  # geom_errorbar(aes(ymin = nu_errmin, ymax = nu_errmax), alpha=0.2) +
  theme_classic()

  # geom_line() + 
  # geom_ribbon(aes(ymin = log(`LL SREP`), ymax = log(`UL SREP`)), alpha = .1) # +

  # geom_point(data = WAin, mapping = aes(logWA, log(Sgen_Mean), colour = lh), alpha=0.2) +
  # geom_errorbar(data = WAin, mapping = aes(ymin = log(`Sgen_2.5%`), ymax = log(`Sgen_97.5%`), colour = lh), alpha=0.2) +

  # geom_point(data = WAbase, mapping = aes(logWA, log(Sgen_Mean), colour = lh)) +
  # geom_errorbar(data = WAbase, mapping = aes(ymin = log(`Sgen_2.5%`), ymax = log(`Sgen_97.5%`), colour = lh )) + 
  # ylab("Log Sgen") +
  # xlab("Log Watershed Area") +
  # theme_classic()
```
