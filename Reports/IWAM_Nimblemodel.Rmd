---
title: "Nimble Model Test"
author: "Tor Kitching"
date: "2024-02-01"
output: html_document
---

# Document Head

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)

# Always remember to add a line between file structures for clean knitting.
```

## Libraries

```{r libraries, echo=FALSE, message=FALSE}
library(tidyverse)
library(nimble)
library(coda)
library(dplyr)

library(rsample)
library(ggplot2)
library(cowplot)
library(gridExtra)
library(reshape2)
library(TMB)
library(zoo)
library(viridis)
library(hrbrthemes)
library(gsl)
library(knitr)

library(geomtextpath)
library(MCMCvis)
```

## Custom Functions

```{r custom library, echo=FALSE}
source(here::here("R/helperFunctions.R"))
  # used for solving Sgen specifically in the Nimble model
    # either using Sgen.fn2 (IWAM original) or Sgen.fn3 (Paul's version)
source(here::here("R/PlotFunctions.R"))
# source(here::here("R/Get_LRP_bs.R")) # This cannot be sourced otherwise it bricks it
source(here::here("R/IWAM_model.R"))
```

```{r LW nimble replicate, echo=FALSE}
## Make lambertW function.
## Use Newton's Method with precision 1e-9 
LambertW <- nimbleFunction(
  # setup = function(){}, ## To run in a nimble model can't have setup code.
  run = function(x = double(), log = integer(0, default = 0)){ 
    if(log == 1) {
      logx <- x
    }else{
        logx <- log(x)
    }
    ## Choose a positive starting value.
    y <- logx
    if(logx < 0) y <- 0
    done <- 0L
    for (i in 1:100) {
      if(done == 0){
        if ( abs( logx - log(y) - y) < 1e-9) {done <- 1}
        y <- y - (y - exp(logx - y)) / (1 + y);                  
      }
    }
    if (done == 0) print("W: failed convergence");
    returnType(double())
    return(y)
  },
  ## Turn this on to build locally and check AD.
  # methods = list(
  #   gr_lambertW = function(x = double(), log = integer(0, default=1)){
  #       ans <- derivs(run(x, log), wrt=1, order=0:2)
  #       returnType(ADNimbleList())
  #       return(ans)
  #     }
  # ),
  buildDerivs = list(run = list(ignore = c('i', 'done')))
)
```

```{r LW0 nimble, echo=FALSE}
FritschIter <- nimbleFunction(
  run = function(x = double(), w = double()){
    MaxEval <- 5
    CONVERGED <- FALSE
    k <- 2.0 / 3.0;
    i <- 0;
    eps <- 2.2204460492503131e-16    
    while (!CONVERGED & i < MaxEval){
      z <- log(x / w) - w
      w1 <- w + 1.0
      q <- 2.0 * w1 * (w1 + k * z)
      qmz <- q - z
      e <- z / w1 * qmz / (qmz - z)
      CONVERGED <- abs(e) <= eps
      w <- w*(1.0 + e)
      i <- i + 1
    }
    returnType(double())
    return(w)
  }
)

LambertW0 <- nimbleFunction(
  setup = function(){},
  run = function(x = double()) {
    eps <- 2.2204460492503131e-16
    returnType(double())
    if (x == Inf) {
      return(Inf);
    } else if (x < -exp(-1)) {
      return(NaN);
    } else if (abs(x - exp(-1)) <= eps) {
      return(-1.0);
    } else if (abs(x) <= 1e-16) {
      ## This close to 0 the W_0 branch is best estimated by its Taylor/Pade
      ## expansion whose first term is the value x and remaining terms are below
      ## machine double precision. See
      ## https://math.stackexchange.com/questions/1700919

      return(x);
    } else {
      w <- 0
      if (abs(x) <= 6.4e-3) {
        ## When this close to 0 the Fritsch iteration may underflow. Instead,
        ## function will use degree-6 minimax polynomial approximation of Halley
        ## iteration-based values. Should be more accurate by three orders of
        ## magnitude than Fritsch's equation (5) in this range.
        return((((((-1.0805085529250425e1 * x + 5.2100070265741278) * x -
               2.6666665063383532) * x + 1.4999999657268301) * x -
               1.0000000000016802) * x + 1.0000000000001752) * x +
               2.6020852139652106e-18);

      } else if (x <= exp(1)) {
        ## Use expansion in Corliss 4.22 to create (2, 2) Pade approximant.
        ## Equation with a few extra terms is:
        ## -1 + p - 1/3p^2 + 11/72p^3 - 43/540p^4 + 689453/8398080p^4 - O(p^5)
        ## This is just used to estimate a good starting point for the Fritsch
        ## iteration process itself.
        
        p <- sqrt(2.0 * (exp(1) * x + 1.0))
        Numer <- (0.2787037037037037 * p + 0.311111111111111) * p - 1.0;
        Denom <- (0.0768518518518518 * p + 0.688888888888889) * p + 1.0;
        w <- Numer / Denom;
      } else {
        ## Use first five terms of Corliss et al. 4.19 */
        w <- log(x)
        L_2 <- log(w)
        L_3 <- L_2 / w
        L_3_sq <- L_3 * L_3
        w <- w - L_2 + L_3 + 0.5 * L_3_sq - L_3 / w + L_3 / (w * w) - 1.5 * L_3_sq /
          w + L_3_sq * L_3 / 3.0;
      }
      return(FritschIter(x, w));
    }
  }
)
```

```{r poor man's LW}
library(gsl)
# RLambertsW <- nimbleRcall(function(x = double(), give = integer(0, default = 0), strict = integer(0, default = 0)){}, Rfun = 'lambert_W0',
#     returnType = double())
# will not give -1 instead of NAN - will still need to thin out the -1's from the chains

RLambertsW <- nimbleRcall(function(x = double()){}, Rfun = 'lambert_W0',
    returnType = double())
```

## S1. Report Introduction

The following model is based upon the Parken et al. 2006, Liermann et al. 2011 model, and Holt el al. 2023, and has been adapted by Paul Van-dan Bates and Tor Kitching. The purpose of this adaptation the following:

1.  Parameterize the Ricker model for SREP (E), based on the Liermann et al. 2011 formulation.
2.  Move away from creating multiple regression models (e.g. between SMSY-WA and SREP-WA), which share inherent information.
3.  Move towards a fully Bayesian approach.
4.  ...



## S2. Datamap

The following function vignette/example is from the IWAM Package Repo hosted publicly through the Pacific Salmon Assess organization available at [this link.](https://github.com/Pacific-salmon-assess/iwam-package)

The package currently utilizes the following data sets:

-   *SRinputfile (Private)*: Private data-sets developed by Parken et al. (2006). Required to run the complete function and is available upon request. To be updated with publicly available data. Life histories are included: 1 = ocean type, 0 = stream type. Contains stock names, year, spawners, recruits, and life history identification.
-   *WatershedArea (Public)*: Internal input containing watershed areas per stock.
-   *CUPars_nBC (Public)*: Input file with Ricker $\alpha$'s without bias correction (CITATION) from Diana Dobson's Run Reconstruction coded in TMB. Has a higher estimate of Ricker $\alpha$ (lower Sgen).
-   *CUPars_wBC (Public)*: Input file with Ricker $\alpha$'s with bias correction (CITATION) from Diana Dobson's Run Reconstruction coded in TMB. Has a higher estimate of Ricker $\alpha$ (lower Sgen).
-   *WCVIStocks (Public)*: User watershed area input file. Must contain watershed areas, life history identification, enhancement identification, and stock name. Aggregation within the IWAM function can be done with both CU and Inlet scales.



## S3. Model Description

Lorem ipsum ...



### S3.a Ricker Model

The following model is based off of Liermann et al. 2011. who supplies the following relationship equations. Equation (\@ref(eq:rickersbase)) shows the baseline Ricker model.

\begin{equation}
  R=Se^{\alpha(1-\frac{S}{S_{REP}})}
  (\#eq:rickersbase)
\end{equation}

where $R$ is the number of recruits,

$S$ is the number of spawners,

and, $S_{REP}$ is the point on the curve where $R = S$.

\begin{equation}
  R_{ij}=S_{ij}exp[r_j(1-\frac{S_{ij}}{E_j})+w_{ij}]
  (\#eq:rickersparameterized)
\end{equation}

where $i$ is year class, 

$j$ is population,

$r_j$ is the $log(slope)$ where $S_{ij} = 0$, with the following prior of:

$$
r_j \sim N(r_m, h_j)
$$,

$$
r_m \sim N(0.6, 0.45)
$$,

$$
h_j \sim N(0, E_{SD})
$$,

$$
E_{SD} \sim Uniform(0,100)
$$,

$E_{ij}$ is equivalent to $S_{REP}$, when $S_{REP} = R_{ij} = S_{ij}$,

and, $w_{ij}$ is the normally distributed independent random variable with mean 0 and population specific standard deviation.

This parameterization of the Ricker's model can be simplified into the following expression. It is important to note that this version excludes the previously defined $w_{ij}$:

\begin{equation}
  log(R/S) = log\alpha_i(1 - \frac{S_i}{E_i})
  (\#eq:nimblericker)
\end{equation}

where $\alpha$ has a hierarchical prior of:

$$
log\alpha \sim N(log\alpha_0, log\alpha_{SD})
$$

$$
log\alpha_0 \sim N(1.5, 2.5)
$$

$$
log\alpha_{SD} \sim Uniform(0, 100)
$$

To avoid confusion, please keep in mind that Liermann substitutes $\alpha$ for $r_j$ as a the productivity parameter. We have opted to maintain the nomenclature of $\alpha$ as per the original Ricker equation.



### S3.b Building the Regression

The Ricker model estimated parameters are then used to build a regression relationship between $E_{ij}$ and watershed area $(WA)$, as follows:

\begin{equation}
  log(E_j) = a + a_DL+j + (b = b_DL_j)logW_j + k_j
  (\#eq:Liermannwaregression)
\end{equation}

which we have modified to read:

\begin{equation}
  log(E_j) = b0 + b_{WA} \cdot logWA_j + logE0_j
  (\#eq:Nimblewaregression)
\end{equation}

where $j$ is population,

$E_{ij}$ is equivalent to $S_{REP}$ (as following with Liermann et al. 2011's formulation),

$b0$ is the intercept of the regression,

$b_{WA}$ is the slope term of the regression,

$logWA_j$ are the logged watershed areas from the synoptic (baseline) data-set,

and, $logE0_j$ is the random error term composed of the following hierarchical priors:

$$
logE0_j \sim N(0,E_{SD})
$$
$$
E_{SD} \sim Uniform(0, 100)
$$

The main difference between the two forms is the coded differentiation of stream versus ocean type regressions. At their core the two create two lines - however the Liermann method creates a parameter that acts is the effective offset between them, where the Nimble model format creates two separate parameters. Both values can be calculated from either model and are redundant.



### S3.c Predictions

The current nimble model calculates $S_{REP}$ (as seen above in the $logE0_j$ equation) and $S_{MSY}$ by the following methods and following Scheurell's Lambert W method for estimation/optimization [CITATION]. This is done by first calculating $\beta$:

$$
\beta = log\alpha/E 
$$ 
or
$$
\beta = log\alpha/S_{REP}
$$

using $\beta$, $S_{MSY}$ is calculated as follows:

$$
S_{MSY} = (1 - LW(1-log\alpha))/\beta
$$

It should be noted that the prior of $\alpha$ is retained from the original evaluation of the Ricker model.



## S4. Data Setup

The data cleaning and setup approach mirrors that described in the IWAM function vignette and worked example. No changes were made to the data or its structure.

```{r data setup}
# Setup data:
srdatwna <- read.csv(here::here("DataIn/SRinputfile.csv"))
WAbase <- read.csv(here::here("DataIn/WatershedArea.csv"))

# Constants:
  # e.g. Enh = True, ...
# remove.EnhStocks <- FALSE
  # Only matters if we do aggregations e.g. by CU or by Inlet
  # In which case just import the IWAM_model code loops into this code chunk starting line 327

srdatwna <- srdatwna %>% filter(Name != "Hoko" & Name != "Hoh") 

stockwna <- srdatwna %>% filter (is.na(Rec) == TRUE) %>% 
  dplyr::select (Stocknumber) %>%  unique() %>% unlist() 

# Remove years with NAs
srdat <- srdatwna %>% filter(Rec != "NA") 
order_test_1 <- srdat %>% filter(Stocknumber == stockwna[1] | 
                             Stocknumber == stockwna[2])
if(max(srdat$Stocknumber) >= stockwna[1]) { # if the max stock number (24)
  for (i in 1:length(stockwna)) { # for  stocks identified with NAs (2)
    len <- length (srdat[which (srdat$Stocknumber == stockwna[i]), ]$yr_num) - 1
    srdat [which (srdat$Stocknumber == stockwna[i]), ]$yr_num <- c (0:len)
  }
}

order_test_2 <- srdat %>% filter(Stocknumber == stockwna[1] | 
                             Stocknumber == stockwna[2])
srdat <- digit_scaling(srdat)
srdat_scale <- srdat %>% dplyr::select(Stocknumber, scale) %>% distinct()
srdat_scale <- srdat_scale$scale 
srdat_cow <- srdat %>% filter(Name == "Cowichan" & 
                                Yr >= 1985 & 
                                Yr !=1986 & 
                                Yr != 1987) # length = 10
n_cow <- length(srdat_cow$Yr)
srdat_cow$yr_num <- 0:(n_cow-1)
srdat <- srdat %>%  filter(Name != "Cowichan") %>% bind_rows(srdat_cow) %>%
  arrange(Stocknumber)
names <- srdat %>% dplyr::select (Stocknumber, Name) %>% distinct()
WAbase <- WAbase %>% full_join(names, by="Name") %>% arrange(Stocknumber)
lifehist <- srdat %>% dplyr::select(Stocknumber, Name, Stream) %>% 
  group_by(Stocknumber) %>% 
  summarize(lh=max(Stream)) %>% 
  arrange (Stocknumber)

data <- list(
  logRS = log(srdat$Rec) - log(srdat$Sp)  
)
```

```{r model inits, echo=FALSE}
inits <- function(){
  list(b0 = c(9, 9),
    bWA = c(0.83, 1),
    logAlpha0 = 1.5,
    logESD = 1,
    logAlphaSD = 1)
}
```

```{r predicted WA, echo=FALSE}
# Insert predictive WA's
WAin <- read.csv(here::here("DataIn/WCVIStocks.csv"))
  # Now includes back-calculated watershed areas

# WAin <- read.csv(here::here("DataIn/WCVI_Parken.csv"))
  # Could just use WCVI_Parken from the beginning here - then I don't have to do this join below

nPred <- length(WAin$WA) # length of predicted watershed areas
logWAPred <-  log(WAin$WA) # predicted watershed areas takes a vector
# logWAPred <- log(WAin$WA_back) # back-calculated water shed areas as a vector
# NOTE: these will be ORDERED - this order must be maintained in order to bind them back in at the end
```

```{r model constants pred, echo=FALSE}
lifehist <- srdat %>% dplyr::select(Stocknumber, Name, Stream) %>% 
    group_by(Stocknumber) %>% 
    summarize(lh=max(Stream))

mean_logWA <- 7.3
constants <- list(
  N_Stk = max(srdat$Stocknumber + 1),
  stk = srdat$Stocknumber + 1,
  N_Obs = nrow(srdat),
  logWAbase = log(WAbase$WA) - mean_logWA, # mean(log(WAIn$WA)) - subtracting is same as division on log scale
  S = srdat$Sp, 
  type = lifehist$lh + 1 # Figure out after which one is which
    # one value per stock in ordering
    # 1 = stream, 2 = ocean
)

pred.line <- data.frame(expand.grid(logWA = 3:16-mean_logWA, type = 1:2))
constants$pred.line <- pred.line

WAin$logWA <- log(WAin$WA) - mean_logWA
WAin$type = WAin$lh + 1
constants$npred.line <- nrow(pred.line)
constants$pred.dat <- WAin[, c("logWA", "type")]
constants$npred <- nrow(WAin)
```



### S4.a Nimble Model

```{r model stock_recruit_srep_biasCor, echo=FALSE}
stock_recruit_srep_biasCor <- nimbleCode({
  
  ## priors
  logAlpha0 ~ dnorm(mean=0.6, sd=0.45)
  # logAlpha0 ~ dnorm(mean=1.5, sd=2.5)
  logAlphaSD ~ dunif(0, 100)
  logESD ~ dunif(0, 100)
    # Can we create regional mean alpha's?
    # Can we use the distribution's used by Diane Dobson - see bootstrap

  ## Based on Parken et al. 2006 (log ( 1/beta ) has a linear relationship with log Watershed Area.
  # b0[1] ~ dflat() 
  b0[1] ~ dnorm(0, tau = 0.001)  ## Flat intercept for stream
  # b0[2] ~ dflat() ## Flat intercept for ocean
  b0[2] ~ dnorm(0, tau = 0.001)

  bWA[1] ~ dnorm(0, tau = 0.01)  ## log Watershed area slope for stream
  bWA[2] ~ dnorm(0, tau = 0.01)  ## log Watershed area slope for ocean

  # sigma ~ dunif(0, 100) # global sigma

  for( i in 1:N_Stk ) { # 25 iterations - number of base stocks
    logAlpha[i] ~ dnorm(logAlpha0, sd = logAlphaSD) ## Random effect for log alpha.
    logE0[i] ~ dnorm(mean = 0, sd = logESD) ## Stock level random effect 

    log_E[i] <- b0[type[i]] + bWA[type[i]]*logWAbase[i] + logE0[i] ## Stock level regression
    # according to Liermann there should be something like:
      # E <- a + a*lifehist[i] + (b + b*lifehist[i])*logWAbase[i] + logE0[i]
    # log_E[i] <- b0 + b0_stream*type[i] + (bWA + bWA_stream*type[i])*logWAbase[i] + logE0[i]
    # where Here lifehist[i] is the life history type (Lj = 0 for stream-type and Lj = 1 for ocean-type).

    tauobs[i] ~ dgamma(0.001, 0.001) # stock specific precision
    E[i] <- exp(log_E[i])
  }

  ## Model and residuals:
  for(i in 1:N_Obs){ # 501 iterations - number of base observations
    logRS_pred[i] <- logAlpha[stk[i]]*(1 - S[i]/E[stk[i]]) # - sigma^2/2
      # remove - sigma^2/2 otherwise the estimations are too big - makes it difficult to interpret the variables

    logRS[i] ~ dnorm( logRS_pred[i], tau = tauobs[stk[i]]) # stock specific precision
    # logRS[i] ~ dnorm( logRS_pred[i], sd = sigma) # global sigma
  }

  ## Pred line:
  for( i in 1:npred.line){
    log_alpha_line[i] ~ dnorm(logAlpha0, sd = logAlphaSD)
    log(E_pred_line[i]) <- b0[pred.line[i,2]] + bWA[pred.line[i,2]]*pred.line[i,1]
    beta_pred_line[i] <- log_alpha_line[i]/E_pred_line[i]
    
    predSmsy_line[i] <- (1 - LambertW(1-log_alpha_line[i], 1))/beta_pred_line[i]
      # doesn't work with RLambertsW ?
    predSgen_line[i] <- -1/beta_pred_line[i]*RLambertsW(-beta_pred_line[i]*predSmsy_line[i]/exp(log_alpha_line[i]))
  }
  
  ## WA predictions
  for( i in 1:npred){
    log_alpha[i] ~ dnorm(logAlpha0, sd = logAlphaSD)
    log(E_pred[i]) <- b0[pred.dat[i,2]] + bWA[pred.dat[i,2]]*pred.dat[i,1]
    beta_pred[i] <- log_alpha[i]/E_pred[i]
    predSmsy[i] <- (1 - LambertW(1-log_alpha[i], 1))/beta_pred[i]
    predSgen[i] <- -1/beta_pred[i]*RLambertsW(-beta_pred[i]*predSmsy[i]/exp(log_alpha[i]))  
  }
  
  
  # ## predWA
  # for ( j in 1:2 ){ # STREAM AND OCEAN TYPE
  #   for( i in 1:nPred ){ # CHANGING - 21 iterations - number of predicted watershed areas
  #   log_alpha[i,j] ~ dnorm(logAlpha0, sd = logAlphaSD)
  #   # logE_re[i] ~ dnorm(mean=0, sd = logESD)
  # 
  #   ## Pred Srep
  #   # log(E_pred[i]) <- b0 + bWA*logWAPred[i] # + logE_re[i]
  #   log(E_pred[i,j]) <- b0[j] + bWA[j]*logWAPred[i] # + logE_re[i]
  #   # log(E_pred[i,j]) <- b0[j] + bWA[j]*logWAbackPred[i]
  #     # Stick without logE_re for now
  #     # remove logE_re - assumes that there is no random effect for a site-specific effect for the prediction - "conditional mean"
  #       # confidence - how uncertain you are about expected value
  #       # prediction - how uncertain you are about predicted new site
  #     
  #   
  #   # If you just want to predict individual sites
  #     # Would need to repeat pattern for every iter of j
  #   # log(E_pred[i]) <- b0[pred_type[i]] + bWA[pred_type[i]]*logWAPred[i]
  #     # E_pred is equal to alpha/beta
  # 
  #   ## Pred Smsy
  #   beta_pred[i,j] <- log_alpha[i,j]/E_pred[i,j] # Pred beta
  #   predSmsy[i,j] <- (1 - LambertW(1-log_alpha[i,j], 1))/beta_pred[i,j]
  #   # predSmsy[i] <- (1 - LambertW(1-log_alpha[i], 1))*E_pred[i]/log_alpha[i]
  #   
  #   ## Pred Sgen
  #   # Double-check Get_LRP_bs for IWAM methods of calculation
  #   # predSgen[i] <- # Sgen equation derivation from: 
  #   # can't call functions from helperFunctions.R into nimble - illegal
  #   # sgen.out <- -1/b.par*gsl::lambert_W0(-b.par*SMSY/a.par)
  #   # predSgen[i,j] <- -1/beta_pred[i,j]*LambertW(-beta_pred[i,j]*predSmsy[i,j]/log_alpha[i,j])
  #     # Check if there are any differences between LambertW and LambertW0
  #     # Two different formulations of Lambert's by Paul
  # }
  # }

})
```

*Notes for Tor: ORIGINAL: take out own alpha distributions - Liermann takes the hierarchical alpha - We know that alpha's differ regionally - so we can either have regional hierarchical means. Want to be able to adjust alpha for Sgen.*

## S5. Model run

Lorem ipsum. ...

```{r model run, warning=FALSE, message=FALSE}
RmodelSrep <- nimbleModel(stock_recruit_srep_biasCor, data = data, constants = constants,
  inits = inits(), buildDerivs=TRUE) # Build derivs if you want MLE

# Running calculate on model
#   [Note] Any error reports that follow may simply reflect missing values in model variables.
# Checking model sizes and dimensions
#   [Note] This model is not fully initialized. This is not an error.
#          To see which variables are not initialized, use model$initializeInfo().
#          For more information on model initialization, see help(modelInitialization).

conf2 <- configureMCMC(RmodelSrep, control = list(adaptFactorExponent = 0.25))
# conf2 <- configureMCMC(RmodelSrep)

conf2$setMonitors(c('logAlpha', 'logAlpha0', 'E', 'log_E', "b0", "bWA", "tauobs",
                    "logAlphaSD", "logESD", "logE0", 
                    "E_pred", "beta_pred", "log_alpha","predSmsy", "predSgen",
                    "log_alpha_line", "beta_pred_line", "predSmsy_line", "E_pred_line", "predSgen_line"))

Rmcmc2 <- buildMCMC(conf2)
Cmodel2 <- compileNimble(RmodelSrep)

Cmcmc2 <- compileNimble(Rmcmc2, project = RmodelSrep)
mcmc.out <- runMCMC(Cmcmc2, niter=50000, nburnin=5000, nchains=3, samplesAsCodaMCMC = TRUE)
```

## S6. Model evaluation and plotting

What plots do I want to show:

1.  Table of WCVI CSAS comparison values for SREP.
2.  Stock-wise comparison of SREP calculations for WCVI stocks between IWAM, Parken, and Nimble? Include confidence intervals/etc..

-   Including:
    a)  Nimble estimates,
    b)  Parken estimates,
    c)  IWAM model estimates.
-   Missing:
    -   Nimble SMSY estimates
    -   Check that Enhanced is removed on Nimble data
    -   Parken estimates for Enh removed
    -   ...

3.  WA regression w/ equation in comment box.

-   For Nimble estimates:
    -   log(WA) vs log(SREP)
    -   log(WA) vs log(SMSY)

```{r mcmc testing}
# consider gathering draws for bayesplots
density <- data.frame(do.call("rbind", mcmc.out))
  # subset out E_preds
  # label them - melt them for ggplot
  # consider thining them to 1000 per chain (4000 total) - posterior's won't be smooth
```

```{r mcmc hists, include=FALSE}
# mcmc.out %>%
#   as_tibble() %>%
#   ggplot() + 
#   geom_histogram(aes(x = chain1[,"E_pred"]), color = "white") + 
#   labs(x = "x axis")
# library(MCMCvis)

# To get common numerical summaries, function MCMCsummary() does the job:
# mcmcsum <- MCMCsummary(object = mcmc.out, round = 2)

# A caterpillar plot to visualise posterior distribution of theta with MCMCplot():
# MCMCplot(object = mcmc.out, params = 'b0')

# Trace plots
MCMCtrace(object = mcmc.out,
          pdf = FALSE, # no export to PDF
          ind = TRUE, # separate density lines per chain
          params = "predSmsy")
```

```{r tidybayes}
# library(tidybayes)
# printbayes <- summarise_draws(mcmc.out)
```

```{r mcmc data retrieval, echo=FALSE, message=FALSE, warning=FALSE}
# could remove the rows that don't have numbers in them
  # get all the Sgen lines and then remove the NaN's from them before doing summary
  # is.NaN



sum.outc <- summary(mcmc.out) # taking mean at real-scale
sum.dfc <- data.frame(do.call("cbind", sum.outc))
Ec <- sum.dfc[substr(rownames(sum.dfc), 1,5) == "log_E",]
Ec <- cbind(Ec, WAbase)
  # bind in WAbase and lifehist so that I can get lh for future usage
Ec <- left_join(Ec, lifehist, by="Stocknumber")

# # cbind back E_pred to the WAin df to get back those predicted SREP'S and CI's
Epred <- sum.dfc[substr(rownames(sum.dfc), 1,7) == "E_pred[",] # 6 refers to the number of characters in the string to search for?
Epred <- cbind(Epred, WAin)
# Mutate in a column for exp(E_pred) to get the final SREP values?
  # Confirm that E_pred is the log form?

# Within Ep (SREP predicted estimates)
Epred <- Epred %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('srep2.5.'= X2.5., 'mean_srep'= Mean, 'srep97.5.'= X97.5., 
         'srepSD' = SD, 'srep50.' = X50., 'srep25.' = X25., 'srep75.' = X75.) %>%
  mutate(logWA = logWA + mean_logWA)

# # cbind back E_pred to the WAin df to get back those predicted SREP'S and CI's
EpredLine <- sum.dfc[substr(rownames(sum.dfc), 1,7) == "E_pred_",] # 6 refers to the number of characters in the string to search for?
EpredLine <- cbind(EpredLine, constants$pred.line)
EpredLine <- EpredLine %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('srep2.5.'= X2.5., 'mean_srep'= Mean, 'srep97.5.'= X97.5., 
         'srepSD' = SD, 'srep50.' = X50., 'srep25.' = X25., 'srep75.' = X75.) %>%
  mutate(logWA = logWA + mean_logWA)

# cbind out the SMSY_pred
ESmsy <- sum.dfc[substr(rownames(sum.dfc), 1,9) == "predSmsy[",] # CHANGE INDEXING
ESmsy  <- cbind(WAin, ESmsy) %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('smsy2.5.'= X2.5., 'mean_smsy'= Mean, 'smsy97.5.'= X97.5., 
         'smsySD' = SD, 'smsy50.' = X50., 'smsy25.' = X25., 'smsy75.' = X75.) %>%
  mutate(logWA = logWA + mean_logWA)  

# cbind out the SMSY_pred
ESmsyLine <- sum.dfc[substr(rownames(sum.dfc), 1,9) == "predSmsy_",] # CHANGE INDEXING
ESmsyLine  <- cbind(ESmsyLine, constants$pred.line) %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('smsy2.5.'= X2.5., 'mean_smsy'= Mean, 'smsy97.5.'= X97.5., 
         'smsySD' = SD, 'smsy50.' = X50., 'smsy25.' = X25., 'smsy75.' = X75.)  %>%
  mutate(logWA = logWA + mean_logWA)

ESgen <- sum.dfc[substr(rownames(sum.dfc), 1,9) == "predSgen[",] # CHANGE INDEXING
ESgen  <- cbind(WAin, ESgen) %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('sgen2.5.'= X2.5., 'mean_sgen'= Mean, 'sgen97.5.'= X97.5., 
         'sgenSD' = SD, 'sgen50.' = X50., 'sgen25.' = X25., 'sgen75.' = X75.) %>%
  mutate(logWA = logWA + mean_logWA)

ESgenLine <- sum.dfc[substr(rownames(sum.dfc), 1,9) == "predSgen_",] # CHANGE INDEXING
ESgenLine  <- cbind(constants$pred.line, ESgenLine) %>% 
  select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
  rename('sgen2.5.'= X2.5., 'mean_sgen'= Mean, 'sgen97.5.'= X97.5., 
         'sgenSD' = SD, 'sgen50.' = X50., 'sgen25.' = X25., 'sgen75.' = X75.) %>%
  mutate(logWA = logWA + mean_logWA)

predLine <- EpredLine %>% left_join(ESmsyLine) %>% left_join(ESgenLine)
predDat <- Epred %>% left_join(ESmsy) %>% left_join(ESgen)


# Now I will have the mean, median, CI's (ALL PREDICTED) and the base WAin dataframe bound together

# #log_alpha's
# logalpha <- sum.dfc[substr(rownames(sum.dfc), 1,10) == "log_alpha[",]
# # alpha <- logalpha[1:21,] # CHANGE INDEXING
# logalpha <- logalpha %>% 
#   select(-Naive.SE, -start, -end, -thin, -nchain, -Time.series.SE) %>% 
#   rename('alpha2.5.'= X2.5., 'mean_alpha'= Mean, 'alpha97.5.'= X97.5., 
#          'alphaSD' = SD, 'alpha50.' = X50., 'alpha25.' = X25., 'alpha75.' = X75.)
```

```{r iwam old sgen calc, echo=FALSE, message=FALSE, warning=FALSE}
# sgen calculation
# log_alpha = alpha
# E_pred = SREP

  # method 1: sgen2
# Sgen.fn2 <- function ( a.par, SREP,  explicit = TRUE , plot=FALSE)
# m1sgen <- Sgen.fn2(exp(alpha$mean_alpha), Srep_pred$mean_srep, 
#                     explicit = TRUE, plot = FALSE)
# SGENcalcs <- purrr::map2_dfr (alpha$mean_alpha, Srep_pred$mean_srep, Sgen.fn2)
  # method 2: sgen3 ala Paul
# Sgen.fn3 <- function ( a.par, SREP,  explicit = TRUE , plot=FALSE)
# const.SMAX <- TRUE
# m2sgen <- Sgen.fn3(exp(alpha$mean_alpha), Srep_pred$mean_srep,
#                    explicit = TRUE, plot=FALSE)
```

### S6. IWAM model Run

```{r iwam and parken model setup, echo=FALSE, message=FALSE, warning=FALSE}
IWAM_WCVI_noEnh <- IWAM_func(WAin = "DataIn/WCVIStocks.csv", # INPUT Data
                       remove.EnhStocks = TRUE, # Enhancement INPUT
                       run.bootstrap = TRUE, # TRUE/FALSE for running the bootstrap process
                       bs_seed = 1, # change the internal seed for the bootstrap
                       bs_nBS = 10, # change the internal number of trials for the bootstrap - 20000
                       plot = FALSE, # print plots to DataOut/
                       est.table = FALSE # store tables - NOT FUNCTIONING
)

SMSY <- IWAM_WCVI_noEnh[[2]] %>%
  filter(RP=='SMSY')
SREP <-IWAM_WCVI_noEnh[[2]] %>%
  filter(RP=='SREP')

Parken_WCVI <- read.csv(here::here("DataIn/WCVI_Parken.csv"))
  # Could just use WCVI_Parken from the beginning here - then I don't have to do this join below
Parken_est <- read.csv(here::here("DataIn/Parkenest.csv"))

eval_dat <- Parken_WCVI %>% left_join(SMSY, by=join_by(Stock)) %>% 
  rename("SMSY" = Value, "UL SMSY" = upr, "LL SMSY" = lwr) %>%
  select(-RP) %>% 
  left_join(SREP, by=join_by(Stock)) %>%
  rename("SREP" = Value, "UL SREP" = upr, "LL SREP" = lwr) %>%
  select(-RP) %>% 
  mutate(PA_UL_SMSY = PA_SMSY + (1.96 * PA_SE_SMSY)) %>% 
  mutate(PA_LL_SMSY = PA_SMSY - (1.96 * PA_SE_SMSY)) %>% 
  mutate(PA_UL_SREP = PA_SREP + (1.96 * PA_SE_SREP)) %>% 
  mutate(PA_LL_SREP = PA_SREP - (1.96 * PA_SE_SREP))
```

```{r acf plot}
SRes <- IWAM_WCVI_noEnh$SRes
Plotacf(SRes)
```

```{r stres plot}
PlotStdResid(SRes)
```

```{r IWAM reg plots}
# prepare data sets
pars <- IWAM_WCVI_noEnh$modelpars
all_Deltas <- IWAM_WCVI_noEnh$all_Deltas
lifehist <- IWAM_WCVI_noEnh$lh
WAbase <- IWAM_WCVI_noEnh$WAbase
pred_lnSREP <- IWAM_WCVI_noEnh$pred_lnSREP
# pred_lnSMSY
pred_lnWA <- IWAM_WCVI_noEnh$pred_lnWA
mod <- "IWAM_Liermann" 

# plotWAregressionSMSY(pars, all_Deltas, srdat, lifehist, WAbase, pred_lnSMSY, 
#                       pred_lnWA = data$pred_lnWA, title1=title_plot, mod)

plotWAregressionSREP(pars, all_Deltas, srdat, lifehist, WAbase, pred_lnSREP, 
                      pred_lnWA = data$pred_lnWA, title1=title_plot, mod)
```

### S6.a Stock-wise comparisons of SREP Plots

```{r stockwise SREP, echo=FALSE, message=FALSE}
# config options: fig.align = 'centre'

# DOUBLE CHECK NEW OBJECT NAMING

# Init plot and add geom for IWAM estimates as baselines
SREP_i <- ggplot(eval_dat, aes(x=Stock, y = SREP)) +
  geom_errorbar(aes(ymax = `UL SREP`, ymin = `LL SREP`), width = 0.2, color='red', position = position_nudge(0.2)) +
  geom_point(color = 'red', position = position_nudge(0.2)) +
  
  theme_classic() + 
  ylab("SREP Estimate") + 
  # ylim(0,50000) +
  theme(axis.text.x = element_text(angle = 90, vjust=0.3, hjust = 1))

# SREP_i

# Add geom for Parken estimates
SREP_p <- SREP_i + 
  geom_point(eval_dat, mapping = aes(x = Stock, y = PA_SREP), position = position_nudge(-0.2)) + 
  geom_errorbar(aes(x = Stock, ymax = PA_UL_SREP, ymin = PA_LL_SREP), width = 0.2, position = position_nudge(-0.2), 
                inherit.aes = FALSE)

# SREP_p

SREP_f <- SREP_p + 
  geom_point(preds, mapping = aes(x = Stock, y = srep50.), color='orange') + # mean_srep or srep50. (median)
  geom_errorbar(data = preds, aes(x = Stock, ymax = `srep97.5.`, ymin = `srep2.5.`), color='orange', width = 0.2, inherit.aes = FALSE)

SREP_f
```

```{r stockwise SMSY, echo=FALSE, message=FALSE}
# config options: fig.align = 'centre'

# DOUBLE CHECK NEW OBJECT NAMING

# Init plot and add geom for IWAM estimates as baselines
SMSY_i <- ggplot(eval_dat, aes(x=Stock, y = SMSY)) +
  geom_errorbar(aes(ymax = `UL SMSY`, ymin = `LL SMSY`), width = 0.2, color='red', position = position_nudge(0.2)) +
  geom_point(color = 'red', position = position_nudge(0.2)) +
  
  theme_classic() + 
  ylab("SMSY Estimate") + 
  # ylim(0,25000) +
  theme(axis.text.x = element_text(angle = 90, vjust=0.3, hjust = 1))

# SMSY_i

# Add geom for Parken estimates
SMSY_p <- SMSY_i + 
  geom_point(eval_dat, mapping = aes(x = Stock, y = PA_SMSY), position = position_nudge(-0.2)) + 
  geom_errorbar(aes(x = Stock, ymax = PA_UL_SMSY, ymin = PA_LL_SMSY), width = 0.2, position = position_nudge(-0.2), 
                inherit.aes = FALSE)

# SMSY_p

# Add final geom for nimble estimates
# SMSY_f <- SMSY_p +
#     geom_point(eval_dat, mapping = aes(x = Stock, y = PA_SMSY), color='orange', position = position_nudge(-0.1)) +
#     geom_errorbar(aes(x = Stock, ymax = PA_UL_SMSY, ymin = PA_LL_SMSY), width = 0.2, position = position_nudge(-0.1),
#                 inherit.aes = FALSE)
SMSY_f <- SMSY_p + 
  geom_point(preds, mapping = aes(x = Stock, y = smsy50.), color='blue') + 
  geom_errorbar(data = preds, aes(x = Stock, ymax = `smsy97.5.`, ymin = `smsy2.5.`), color='blue', width = 0.2, inherit.aes = FALSE)

SMSY_f
```

```{r synoptic stockwise}
# Parken data: Parken_est
# IWAM data: ???
# Nimble data: Ec (Srep only)

# Base of Nimble estimates
SREP_synN <- ggplot(Ec, aes(x=Name, y = exp(X50.))) +
  geom_errorbar(aes(ymax = exp(X97.5.), ymin = exp(X2.5.)), width = 0.2, color='blue', 
                position = position_nudge(0.2)) +
  geom_point(color = 'blue', position = position_nudge(0.2)) +
  theme_classic() + 
  ylab("SREP Estimate") + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.3, hjust = 1))

# SREP_synN

# Add geom for Parken point estimates (no errorbars)
SREP_synP <- SREP_synN + 
  geom_point(Parken_est, mapping = aes(x = Stockname, y = SREP, inherit.aes = FALSE), 
             position = position_nudge(-0.2))

# SREP_synP

# Iwam synoptic data retreival
iwam_synoptic <- IWAM_WCVI_noEnh$modelpars[IWAM_WCVI_noEnh$modelpars$Param == "lnSREP", ]

# Add geom for IWAM points + errorbars
SREP_synplot <- SREP_synP +
  geom_point(iwam_synoptic, mapping = aes(x = Name, y = exp(Estimate)), color='red') + # mean_srep or srep50. (median)
  geom_errorbar(data = iwam_synoptic, aes(x = Name, ymax = exp(Estimate + 1.96 * Std..Error),
                                          ymin = exp(Estimate - 1.96 * Std..Error)),
                                          color='red', width = 0.2,inherit.aes = FALSE)

SREP_synplot
```

### S6.b WA Regression Plots

Important to note that because there is no error on the prediction terms - one is essentially drawing from the regression line created. This can best be seen by plotting log predicted 'mean' Srep against log watershed area - which produces a straight line. This line is controlled by the slope and intercept estimated by the model posterior.

```{r fixing WAreg}
# Fixing that the points are on the line
  # bWA and b0 are describing the mean mu on the log-scale and therefore need
  # to be transformed correctly - best done directly from the mcmc outputs.
  # Morale of the story is that summary() is a trap.
  # Watch what when you take means.
pred.WA <- data.frame()
out.b0 <- do.call('rbind', mcmc.out[, c("b0[1]", "b0[2]")])
out.bwa <- do.call('rbind', mcmc.out[, c("bWA[1]", "bWA[2]")])
out.alpha <- do.call('rbind', mcmc.out[, c("logAlpha0", "logAlphaSD")])
for( logA in 2:20){
  for(i in 1:2){
    tmp <- exp(out.b0[,i] + out.bwa[,i]*logA)
    type <- c("stream", "ocean")[i]
    pred.WA <- rbind(pred.WA, data.frame(logWA = logA, type = type, mean = mean(tmp), median = median(tmp), 
                                lower = quantile(tmp, 0.025), upper = quantile(tmp, 0.975)))
  }
}

pred.WA %>% ggplot(aes(x=logWA, y = log(mean), col = type, fill=type)) +
    geom_line() + 
    geom_ribbon(aes(ymin = log(lower), ymax = log(upper)), alpha = .2) +
    geom_point(data = preds, mapping=aes(x = log(WA), y = log(mean_srep)), shape=1) +
    geom_point(data = Ec, mapping=aes(x = log(WA), y = Mean)) +
    theme_classic()
```

```{r bwa values, echo=FALSE, message=FALSE}
intercept <- sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"]
int_upper <- sum.dfc[grepl("b0", row.names(sum.dfc )),"X2.5."]
int_lower <- sum.dfc[grepl("b0", row.names(sum.dfc )),"X97.5."]

slope <-  sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]
slope_upper <- sum.dfc[grepl("bWA", row.names(sum.dfc )),"X2.5."]
slope_lower <- sum.dfc[grepl("bWA", row.names(sum.dfc )),"X97.5."]
```

```{r baseline WA nimble, echo=FALSE, message=FALSE}
# Regression of WAbase and Srep
ggplot(data = Ec, aes(x = log(WA), y = Mean, color=factor(lh+1))) +
  geom_point() + # colour points based on whether they are stream or ocean (lh)
  geom_errorbar(aes(ymin = X2.5., ymax = X97.5. )) +
  
  geom_point(data = preds, mapping=aes(x = log(WA), y = log(mean_srep)), shape=1) + 
  geom_errorbar(mapping=aes(ymin = log(srep2.5.), ymax = log(srep97.5.), x = log(WA)),
                color='red', linetype='dashed',
                data = preds, inherit.aes = FALSE) +
  theme_classic() +
  # geom_abline(intercept = sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"],
  #   slope = sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]) +
  geom_abline(intercept = intercept[1],
              slope = slope[1],
              color = 'black') +
  # add ribbon?
    # will require two additional lines for upper and lower limit
    # and then a geom_ribbon
  geom_abline(intercept = int_upper[1],
              slope = slope_upper[1],
              color = 'black',
              linetype='dashed') +
  geom_abline(intercept = int_lower[1],
              slope = slope_lower[1],
              color = 'black',
              linetype='dashed') +
  
  geom_abline(intercept = intercept[2], 
              slope = slope[2],
              color = 'red') +
  geom_abline(intercept = int_upper[2],
              slope = slope_upper[2],
              color = 'red',
              linetype='dashed') +
  geom_abline(intercept = int_lower[2],
              slope = slope_lower[2],
              color = 'red',
              linetype='dashed') +
  # add ribbon
  # geom_abline(intercept = 0,
  #             slope = 1,
  #             linetype = 'dashed') +
  scale_color_manual(values = c("black", "red"), labels = c("stream", "ocean")) +
  labs(color = "Life history") +
  xlim(2,13) +
  ylim(3,15) +
  ylab("Log Srep") +
  xlab("Log Watershed Area")
# Color them for ocean/vs stream type
# How does it look with the logE_re term
```

```{r baseline WA IWAM, echo=FALSE, message=FALSE}
# Regression of WAbase and Srep
# IWAM Data
  # eval_dat$SREP

ggplot(data = eval_dat, aes(x = log(WA), y = log(SREP), color=factor(lh+1))) +
  geom_point() + # colour points based on whether they are stream or ocean (lh)
  geom_errorbar(aes(ymin = log(`LL SREP`), ymax = log(`UL SREP`) )) +
  
  # geom_point(data = preds, mapping=aes(x = log(WA), y = log(mean_srep)), shape=1) + 
  # geom_errorbar(mapping=aes(ymin = log(srep2.5.), ymax = log(srep97.5.), x = log(WA)),
  #               color='red', linetype='dashed',
  #               data = preds, inherit.aes = FALSE) +
  theme_classic() +
  # geom_abline(intercept = sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"],
  #   slope = sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]) +
  # geom_abline(intercept = intercept[1],
  #             slope = slope[1],
  #             color = 'black') +
  # # add ribbon?
  #   # will require two additional lines for upper and lower limit
  #   # and then a geom_ribbon
  # geom_abline(intercept = int_upper[1],
  #             slope = slope_upper[1],
  #             color = 'black',
  #             linetype='dashed') +
  # geom_abline(intercept = int_lower[1],
  #             slope = slope_lower[1],
  #             color = 'black',
  #             linetype='dashed') +
  # 
  # geom_abline(intercept = intercept[2], 
  #             slope = slope[2],
  #             color = 'red') +
  # geom_abline(intercept = int_upper[2],
  #             slope = slope_upper[2],
  #             color = 'red',
  #             linetype='dashed') +
  # geom_abline(intercept = int_lower[2],
  #             slope = slope_lower[2],
  #             color = 'red',
  #             linetype='dashed') +
  # add ribbon
  # geom_abline(intercept = 0,
  #             slope = 1,
  #             linetype = 'dashed') +
  
  scale_color_manual(values = c("black", "red"), labels = c("stream", "ocean")) +
  labs(color = "Life history") +
  # xlim(2,13) +
  # ylim(3,15) +
  ylab("Log Srep") +
  xlab("Log Watershed Area")
# Color them for ocean/vs stream type
# How does it look with the logE_re term
```

```{r logWA reg plot Sgen, echo=FALSE, message=FALSE}
reg_mean_sgen <- ggplot(data = predDat, aes(x = log(WA), y = log(mean_sgen))) +
  geom_point() +
  geom_errorbar(aes(ymin = log(srep2.5.), ymax = log(srep97.5.) )) +
  theme_classic() +
  # geom_abline(intercept = sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"],
  #   slope = sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]) +
  ylab("Log Srep") +
  xlab("Log Watershed Area")
```

```{r logWA regression plots, echo=FALSE, message=FALSE}
reg_mean <- ggplot(data = preds, aes(x = log(WA), y = log(mean_srep))) +
  geom_point() +
  geom_errorbar(aes(ymin = log(srep2.5.), ymax = log(srep97.5.) )) +
  theme_classic() +
  # geom_abline(intercept = sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"],
  #   slope = sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]) +
  ylab("Log Srep") +
  xlab("Log Watershed Area")

# reg_mean

reg_f <- reg_mean + 
  geom_point(data = preds, mapping = aes(x = log(WA), y = log(srep50.)), color='orange') # +
  # geom_errorbar(aes(ymin = log(srep2.5.), ymax = log(srep97.5.) ))

reg_f
```

```{r pauls plots, include=FALSE, echo=FALSE}
# ggplot(data = Ec, aes(x = log(WA), y = Mean)) +
#   geom_point() + 
#   geom_errorbar(aes(ymin = X2.5., ymax = X97.5. )) + 
#   theme_classic() + 
#   geom_abline(intercept = sum.dfc[grepl("b0", row.names(sum.dfc )),"Mean"], 
#     slope = sum.dfc[grepl("bWA", row.names(sum.dfc )), "Mean"]) + 
#   ylab("Log Srep") + 
#   xlab("Log Watershed Area")
# 
# alphac <- sum.dfc[substr(rownames(sum.dfc), 1,9) == "logAlpha[",]
# alphac <- cbind(alphac, WAbase)
# 
# hist(alphac$Mean)
# 
# plot(outc[, "logAlpha[1]"])
# plot(outc[, "logAlpha[1]"])
```



### S6.c CSAS Table comparisons

```{r nimble table, include=FALSE, echo=FALSE, message=FALSE}
# NON FUNCTION - change the object names
# combo <- data.frame(SGEN_noenh, SREP_noenh)
# combo <- combo %>% 
#   select(-Stock.1) %>%
#   rename("SGEN LQ" = Lower.Quantile, "SGEN UQ" = Upper.Quantile,
#   "SREP LQ" = Lower.Quantile.1, "SREP UQ" = Upper.Quantile.1)
# # print(head(combo))
# kable(combo, caption = "Estimated SREP, SGEN, and SMSY with Enhanced Stocks")
```

```{r CSAS table, include=FALSE, echo=FALSE, message=FALSE}
# Population_csas <- c('Barkley' , 'Clayoquot' , 'Kyuquot' , 'Quatsino' , 'Nootka/Esperanza')
# 
# SGEN_csas <- c(120, 1400, 1000, 220, 650)
# SGEN_ul_csas <- c(28, 350, 240, 55, 160)
# SGEN_ll_csas <- c(430, 4300, 3200, 760, 2100)
# 
# SREP_csas <- c(640, 7300, 5300, 1200, 3400)
# SREP_ul_csas <- c(290, 4100, 2900, 570, 1800)
# SREP_ll_csas <- c(1400, 13000, 9600, 2400, 6300)
# 
# CSAS <- data.frame(Population_csas, SGEN_csas, SGEN_ul_csas, SGEN_ll_csas, SREP_csas, SREP_ul_csas, SREP_ll_csas)
# CSAS <- CSAS %>% 
#   rename('Population'=Population_csas, 
#          'SGEN'=SGEN_csas, 'SGEN Upper 95% CL'=SGEN_ul_csas, 'SGEN Lower 95% CL'=SGEN_ll_csas, 
#          'SREP'=SREP_csas, 'SREP Upper 95% CL'=SREP_ul_csas, 'SREP Lower 95% CL'=SREP_ll_csas)
# 
# kable(CSAS, caption = "CSAS SREP AND SGEN Estimates")
```
